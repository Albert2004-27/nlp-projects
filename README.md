# WattBot 2025 - Team Attention Please üîã

<div align="center">

[![Competition](https://img.shields.io/badge/Kaggle-WattBot%202025-20BEFF?style=for-the-badge&logo=kaggle)](https://kaggle.com/competitions/WattBot2025)
[![Team](https://img.shields.io/badge/Team-Attention%20Please-FF6B6B?style=for-the-badge)]()
[![System](https://img.shields.io/badge/System-HERO-9B59B6?style=for-the-badge)]()
[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)]()

### **HERO: Hierarchical Evidence Retrieval & Optimization**
*Evidence-based Energy Estimation for AI Workloads using Retrieval Augmented Generation*

[üìÑ Technical Report](docs/WattBot2025_AttentionPlease_Technical_Report.pdf) | [üìä Presentation](docs/WattBot2025_AttentionPlease_Presentation.pdf)

</div>

---

## üìå Competition Overview

**WattBot 2025** challenges participants to build RAG systems that extract credible environmental impact estimates from academic literature. Our system must provide:

- üéØ Concise, citation-backed answers
- üìö Document IDs and supporting evidence
- ‚ùå Explicit handling of unanswerable questions

### Dataset
- **32 scholarly articles** (2019-2025) on AI's environmental impact
- **Topics**: Energy consumption, carbon emissions, water usage, sustainability
- **Question Types**: Numeric values, categorical terms, True/False

### Evaluation Metrics
| Component | Weight | Criteria |
|-----------|--------|----------|
| `answer_value` | 75% | Numeric accuracy (¬±0.1% tolerance) or exact categorical match |
| `ref_id` | 15% | Jaccard overlap with ground truth citations |
| `is_NA` | 10% | Proper handling of unanswerable questions |

---

## üèÜ Results

| Metric | Score |
|--------|-------|
| **Final WattBot Score** | [To be updated] |
| **Public Leaderboard Rank** | [To be updated] |
| **Private Leaderboard Rank** | [To be updated] |

---

## üéØ Solution Architecture

Our **HERO (Hierarchical Evidence Retrieval & Optimization)** pipeline consists of four key components:

### üåü What Makes HERO Different?

| Feature | HERO Approach | Traditional RAG |
|---------|---------------|-----------------|
| **Document Structure** | Preserves hierarchy (Doc‚ÜíSection‚ÜíLeaf) | Flat chunking |
| **Retrieval Strategy** | Hybrid (BM25 + Dense) with reranking | Single-method |
| **Storage** | SQLite single-file | External vector DBs |
| **Visual Processing** | OCR + Table extraction | Text-only |
| **Citation Handling** | Multi-document evidence aggregation | Single-source |

### 1. **Document Processing** üìÑ
```
PDF Documents ‚Üí Hierarchical Parsing ‚Üí Structured Chunks
```
- **Parser**: PyMuPDF for text extraction
- **Structure Preservation**: Document ‚Üí Section ‚Üí Paragraph hierarchy
- **Visual Processing**: OCR for tables/figures containing critical metrics
- **Chunking Strategy**: ~150-180 words per chunk with overlap

### 2. **Hybrid Retrieval System** üîç
```
Query ‚Üí [BM25 + Dense Embeddings] ‚Üí Top-K Chunks
```
- **BM25**: Keyword-based retrieval for precise term matching
- **Dense Retrieval**: Sentence-Transformers for semantic search
- **Reranking**: Weighted combination (Œ±=0.6 for BM25, Œ≤=0.4 for Dense)
- **Storage**: SQLite for single-file reproducibility

### 3. **Answer Generation** ü§ñ
```
Retrieved Context + Question ‚Üí Gemini 2.5 Pro ‚Üí Structured Answer
```
- **Model**: Google Gemini 2.5 Pro (gemini-2.0-flash-exp)
- **Prompt Engineering**: Few-shot examples + structured output format
- **Rate Limiting**: Exponential backoff for API stability
- **Output**: JSON with answer, citations, and supporting evidence

### 4. **Post-Processing** ‚úÖ
```
Raw Answers ‚Üí Validation ‚Üí Unit Normalization ‚Üí Final Submission
```
- Numeric validation (¬±0.1% tolerance)
- Citation format checking
- "Unable to answer" fallback handling

---

## üîë Key Technical Insights

Based on our experimental results:

### 1. **Structure > Chunking**
> üí° **Lesson**: Do not treat PDFs as flat text strings. Preserving document hierarchy (Doc ‚Üí Section ‚Üí Leaf) allows precise targeting.

**Implementation**:
- Tracked section headers and metadata
- Maintained parent-child relationships in chunks
- Enabled context-aware retrieval

### 2. **Engineering Simplicity**
> üí° **No Vector DB**: Using SQLite as a single-file datastore reduces complexity and ensures reproducibility.

**Benefits**:
- ‚úÖ No external dependencies
- ‚úÖ Easy version control
- ‚úÖ Portable across environments
- ‚úÖ Resilient to API rate limits with backoff logic

### 3. **Visuals are Data, Not Noise**
> üí° **Lesson**: Critical energy metrics are often hidden in charts. We must process images (via Vision Models/OCR), not just filter them out.

**Strategy**:
- Extracted figures/tables as separate chunks
- Used OCR for table data
- Linked visual content to text context

---

## üìÅ Repository Structure

```
.
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ v1_gemini_2_5pro.ipynb              # Initial pipeline implementation
‚îÇ   ‚îî‚îÄ‚îÄ Gemini_2_5_pro_Êñ∞pipeline_0_821.ipynb  # Optimized version (0.821 score)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ WattBot2025_AttentionPlease_Technical_Report.pdf
‚îÇ   ‚îî‚îÄ‚îÄ WattBot2025_AttentionPlease_Presentation.pdf
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ metadata.csv                         # Document index
‚îÇ   ‚îú‚îÄ‚îÄ train_QA.csv                         # Training Q&A pairs
‚îÇ   ‚îî‚îÄ‚îÄ test_Q.csv                           # Test questions
‚îú‚îÄ‚îÄ src/                                     # (Optional) Modular code
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îî‚îÄ‚îÄ submission.csv                       # Final predictions
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start

### Prerequisites
```bash
pip install pymupdf rank-bm25 sentence-transformers google-generativeai pandas
```

### Environment Setup
```bash
# Set Google API Key
export GOOGLE_API_KEY="your-api-key-here"
```

### Run the Pipeline
```bash
# Open the notebook
jupyter notebook notebooks/Gemini_2_5_pro_Êñ∞pipeline_0_821.ipynb

# Or run as script (if converted)
python src/main.py --input data/test_Q.csv --output outputs/submission.csv
```

### Expected Output
```
‚úÖ Document Parsing: 32/32 papers processed
üìä Total chunks: 1778
üîç Hybrid Search: BM25 + Dense retrieval ready
ü§ñ Generating answers...
Progress: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250
üíæ Submission saved: outputs/submission.csv
```

---

## üìä Performance Breakdown

### Retrieval Performance

| Metric | Baseline | HERO (Ours) | Œî |
|--------|----------|-------------|---|
| **Recall@K (Coverage)** |
| Recall@1 | 79.49% | **80.49%** | +1.00% |
| Recall@3 | 89.74% | **89.80%** | +0.06% |
| Recall@5 | 92.31% | **92.68%** | +0.37% |
| Recall@10 | 92.31% | **95.12%** | +2.81% |
| **nDCG@K (Ranking Quality)** |
| nDCG@1 | 0.7949 | **0.8049** | +0.0100 |
| nDCG@3 | 0.8497 | **0.8502** | +0.0005 |
| nDCG@5 | 0.8564 | **0.8584** | +0.0020 |
| nDCG@10 | 0.8617 | **0.8665** | +0.0048 |
| **Overall Accuracy** |
| MRR | 0.8526 | **0.8560** | +0.0034 |

### Answer Accuracy by Question Type
| Type | Count | Accuracy |
|------|-------|----------|
| Numeric | ~40% | [TBD] |
| Categorical | ~35% | [TBD] |
| True/False | ~15% | [TBD] |
| Unanswerable | ~10% | [TBD] |

---

## üõ†Ô∏è Technical Stack

| Component | Technology |
|-----------|-----------|
| **Document Parsing** | PyMuPDF, regex |
| **Keyword Retrieval** | BM25 (rank-bm25) |
| **Dense Retrieval** | Sentence-Transformers (all-MiniLM-L6-v2) |
| **Vector Storage** | SQLite |
| **LLM** | Google Gemini 2.5 Pro |
| **Development** | Python 3.10+, Jupyter Notebook |

---

## üß™ Experimental Findings

### What Worked ‚úÖ
1. **Hybrid retrieval** outperformed single-method approaches by 12%
2. **Hierarchical chunking** improved citation accuracy by preserving context
3. **Few-shot prompting** reduced hallucinations in numeric answers
4. **Exponential backoff** handled API rate limits gracefully

### What Didn't Work ‚ùå
1. Pure dense retrieval missed exact term matches (e.g., "BERT-base")
2. Large chunks (>300 words) diluted relevant information
3. Zero-shot prompting generated inconsistent citation formats

### Ablation Study
| Configuration | WattBot Score | Œî |
|---------------|---------------|---|
| BM25 only | 0.64 | -0.18 |
| Dense only | 0.71 | -0.11 |
| **HERO (Hybrid)** | **0.82** | **baseline** |
| HERO + Visual processing | 0.85 | +0.03 |

---

## üìö Key References

1. **Competition Dataset**: Endemann, C., Paul, D. J., & Zhao, A. (2025). *WattBot 2025*. Kaggle.
2. **Retrieval Methods**: Robertson, S., & Zaragoza, H. (2009). *The Probabilistic Relevance Framework: BM25 and Beyond*.
3. **RAG Survey**: Gao, Y., et al. (2023). *Retrieval-Augmented Generation for Large Language Models: A Survey*.

---

## üë• Team Members

**Team Attention Please**

- **Shao-Hua Wu**‚Ä° - Document Processing & Retrieval System
- **Xie-Pei Ju**‚Ä° - Hybrid Search Implementation & Optimization
- **Bo-Hao Chen**‚Ä° - Answer Generation & Prompt Engineering
- **Yi-Chen Hsiao**‚àó - Project Lead & System Architecture
- **Yi-Yang Xue**‚Ä† - Evaluation & Performance Analysis

<sup>‚Ä° Equal contribution</sup>

---

## üìù Citation

If you find our approach useful, please cite:

```bibtex
@misc{wattbot2025_hero,
  title={HERO: Hierarchical Evidence Retrieval \& Optimization for WattBot 2025},
  author={Wu, Shao-Hua and Ju, Xie-Pei and Chen, Bo-Hao and Hsiao, Yi-Chen and Xue, Yi-Yang},
  year={2025},
  howpublished={\url{https://github.com/your-repo/wattbot2025-hero}}
}
```

---

## üìÑ License

This project is released under the MIT License. See [LICENSE](LICENSE) for details.

---

## üôè Acknowledgments

- Competition organizers at ML+X, University of Wisconsin-Madison
- Google for Gemini API access
- Open-source community for tools and libraries

---

<div align="center">

**‚≠ê If you found this helpful, please consider starring the repository!**

[![Star on GitHub](https://img.shields.io/github/stars/your-username/wattbot2025?style=social)](https://github.com/your-username/wattbot2025)

</div>
