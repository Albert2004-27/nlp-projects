{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 105181,
          "databundleVersionId": 13704170,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "228ed790a97744d7b974031b41e9f54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98a68eb829a94aa8bacfe74485c3b3cd",
              "IPY_MODEL_4b20b5526255448991ea10bba03007f6",
              "IPY_MODEL_e1c9a26e5fe74199bd7a25408e27d000"
            ],
            "layout": "IPY_MODEL_36cf7929b17542d69be4a6554cff083d"
          }
        },
        "98a68eb829a94aa8bacfe74485c3b3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11b3470c69b34df8ae1d08d0f1de08a6",
            "placeholder": "​",
            "style": "IPY_MODEL_ac326d8afaec4caab9e6cf187dccdc07",
            "value": "生成中: 100%"
          }
        },
        "4b20b5526255448991ea10bba03007f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4202c542e45747f9bca70167c8587cd2",
            "max": 282,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3c0dda798f3402ea81f3b576c1c748e",
            "value": 282
          }
        },
        "e1c9a26e5fe74199bd7a25408e27d000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16f66c94bc604c0a8d9e9c0c5c619818",
            "placeholder": "​",
            "style": "IPY_MODEL_d22dd860cb164085a04ceeac2a963518",
            "value": " 282/282 [55:25&lt;00:00, 11.98s/it]"
          }
        },
        "36cf7929b17542d69be4a6554cff083d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b3470c69b34df8ae1d08d0f1de08a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac326d8afaec4caab9e6cf187dccdc07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4202c542e45747f9bca70167c8587cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c0dda798f3402ea81f3b576c1c748e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16f66c94bc604c0a8d9e9c0c5c619818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22dd860cb164085a04ceeac2a963518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eec01bec2634f54917143b8818328e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2972a2b9011c438cbbacc9917ec698eb"
            ],
            "layout": "IPY_MODEL_91e2c0c274c441909a06222f95a409ca"
          }
        },
        "d187589e82e34cca904029a9be7f1a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e55e4b8306364d53b4465dfa0a72c1cc",
            "placeholder": "​",
            "style": "IPY_MODEL_e099942dd141424abc6086a07edfbca2",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "e56c67aa654542b9a296988e0308c2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6d6114f8b0c942d7b23f544e3f1a0825",
            "placeholder": "​",
            "style": "IPY_MODEL_277ae893a6574f34b6e1aa3243d9e8f5",
            "value": "pianotakodachi"
          }
        },
        "e2ba7929612147d8b6fa3be16d5834eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dcd11106c00c442ca89220e3e5ec7f96",
            "placeholder": "​",
            "style": "IPY_MODEL_7aeb0b69cdb945468abc99400f5c2a18",
            "value": ""
          }
        },
        "94c3caafe1b34d809154a5cd008958cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_01bca03604bc4a0ab99bcb46ac1c94d0",
            "style": "IPY_MODEL_d0cb15101b1c4daf8ba054270cde9a74",
            "tooltip": ""
          }
        },
        "6590a3894f5645239c5200b5ae887f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_306783fb41854267a61cbddf2218a6f7",
            "placeholder": "​",
            "style": "IPY_MODEL_6e34d48052054e2e9079cc7a43907735",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "91e2c0c274c441909a06222f95a409ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e55e4b8306364d53b4465dfa0a72c1cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e099942dd141424abc6086a07edfbca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d6114f8b0c942d7b23f544e3f1a0825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "277ae893a6574f34b6e1aa3243d9e8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcd11106c00c442ca89220e3e5ec7f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aeb0b69cdb945468abc99400f5c2a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01bca03604bc4a0ab99bcb46ac1c94d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0cb15101b1c4daf8ba054270cde9a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "306783fb41854267a61cbddf2218a6f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e34d48052054e2e9079cc7a43907735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dd54f42470d4f7093bf9db8ccc053e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d319fcb3bcc4541b82ee342254905fa",
            "placeholder": "​",
            "style": "IPY_MODEL_6aa24200a09a4cfab4daf7899f555ec8",
            "value": "Connecting..."
          }
        },
        "6d319fcb3bcc4541b82ee342254905fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa24200a09a4cfab4daf7899f555ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2972a2b9011c438cbbacc9917ec698eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc7e4ac357af4e39ba2ddeb7f66b93bc",
            "placeholder": "​",
            "style": "IPY_MODEL_bde2c2cd6fc44d0f953c0a0c72578a04",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "bc7e4ac357af4e39ba2ddeb7f66b93bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde2c2cd6fc44d0f953c0a0c72578a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Final Report**"
      ],
      "metadata": {
        "id": "ROIlY9eI2bRO"
      }
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "9eec01bec2634f54917143b8818328e1",
            "d187589e82e34cca904029a9be7f1a98",
            "e56c67aa654542b9a296988e0308c2ed",
            "e2ba7929612147d8b6fa3be16d5834eb",
            "94c3caafe1b34d809154a5cd008958cc",
            "6590a3894f5645239c5200b5ae887f72",
            "91e2c0c274c441909a06222f95a409ca",
            "e55e4b8306364d53b4465dfa0a72c1cc",
            "e099942dd141424abc6086a07edfbca2",
            "6d6114f8b0c942d7b23f544e3f1a0825",
            "277ae893a6574f34b6e1aa3243d9e8f5",
            "dcd11106c00c442ca89220e3e5ec7f96",
            "7aeb0b69cdb945468abc99400f5c2a18",
            "01bca03604bc4a0ab99bcb46ac1c94d0",
            "d0cb15101b1c4daf8ba054270cde9a74",
            "306783fb41854267a61cbddf2218a6f7",
            "6e34d48052054e2e9079cc7a43907735",
            "1dd54f42470d4f7093bf9db8ccc053e1",
            "6d319fcb3bcc4541b82ee342254905fa",
            "6aa24200a09a4cfab4daf7899f555ec8",
            "2972a2b9011c438cbbacc9917ec698eb",
            "bc7e4ac357af4e39ba2ddeb7f66b93bc",
            "bde2c2cd6fc44d0f953c0a0c72578a04"
          ]
        },
        "id": "B_enuW8dEWvy",
        "outputId": "e36b2975-6ecc-49eb-ba09-69908431c059"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eec01bec2634f54917143b8818328e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pdfplumber pdf2image pytesseract\n",
        "!apt-get -y install poppler-utils tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bBntqMPF1Wu",
        "outputId": "799db38e-a0bf-4c4a-f831-b479092b656b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "wattbot2025_path = kagglehub.competition_download(\"WattBot2025\")\n",
        "print(\"Downloaded to:\", wattbot2025_path)\n",
        "\n",
        "DATA_DIR  = Path(wattbot2025_path)\n",
        "INPUT_DIR = DATA_DIR\n",
        "\n",
        "WORK_DIR = Path(\"/content/wattbot_working\")\n",
        "WORK_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "pdf_dir   = WORK_DIR / \"pdfs\"\n",
        "text_dir  = WORK_DIR / \"texts\"\n",
        "chunk_dir = WORK_DIR / \"chunks\"\n",
        "\n",
        "for d in [pdf_dir, text_dir, chunk_dir]:\n",
        "    d.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "metadata = pd.read_csv(INPUT_DIR / \"metadata.csv\", encoding=\"latin-1\")\n",
        "train_qa = pd.read_csv(INPUT_DIR / \"train_QA.csv\", encoding=\"latin-1\")\n",
        "test_q   = pd.read_csv(INPUT_DIR / \"test_Q.csv\",  encoding=\"latin-1\")\n",
        "\n",
        "print(metadata.head())\n",
        "print(train_qa.head())\n",
        "print(test_q.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-13T07:16:50.840625Z",
          "iopub.execute_input": "2025-11-13T07:16:50.840968Z",
          "iopub.status.idle": "2025-11-13T07:16:50.871775Z",
          "shell.execute_reply.started": "2025-11-13T07:16:50.840943Z",
          "shell.execute_reply": "2025-11-13T07:16:50.870571Z"
        },
        "id": "_7QZTM1aEWv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b597c2-b073-4db5-e080-6aec67c032ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/competitions/data/download-all/WattBot2025...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22.9k/22.9k [00:00<00:00, 25.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Downloaded to: /root/.cache/kagglehub/competitions/WattBot2025\n",
            "            id    type                                              title  \\\n",
            "0   amazon2023  report                  2023 Amazon Sustainability Report   \n",
            "1     chen2024   paper  Efficient Heterogeneous Large Language Model D...   \n",
            "2    chung2025   paper  The ML.ENERGY Benchmark: Toward Automated Infe...   \n",
            "3  cottier2024   paper    The Rising Costs of Training Frontier AI Models   \n",
            "4    dodge2022   paper  Measuring the Carbon Intensity of AI in Cloud ...   \n",
            "\n",
            "   year                                           citation  \\\n",
            "0  2023  Amazon Staff. (2023). Amazon Sustainability Re...   \n",
            "1  2024  Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...   \n",
            "2  2025  Jae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...   \n",
            "3  2024  Ben Cottier, Robi Rahman, Loredana Fattorini, ...   \n",
            "4  2022  Jesse Dodge, Taylor Prewitt, Remi Tachet Des C...   \n",
            "\n",
            "                                                 url  \n",
            "0  https://sustainability.aboutamazon.com/2023-am...  \n",
            "1                   https://arxiv.org/pdf/2405.01814  \n",
            "2                   https://arxiv.org/pdf/2505.06371  \n",
            "3                   https://arxiv.org/pdf/2405.21015  \n",
            "4                   https://arxiv.org/pdf/2206.05229  \n",
            "  ï»¿id                                           question  \\\n",
            "0  q003  What is the name of the benchmark suite presen...   \n",
            "1  q009  What were the net CO2e emissions from training...   \n",
            "2  q054  What is the model size in gigabytes (GB) for t...   \n",
            "3  q062  What was the total electricity consumption of ...   \n",
            "4  q075  True or False: Hyperscale data centers in 2020...   \n",
            "\n",
            "                                              answer         answer_value  \\\n",
            "0                            The ML.ENERGY Benchmark  ML.ENERGY Benchmark   \n",
            "1                                          4.3 tCO2e                  4.3   \n",
            "2                                            64.7 GB                 64.7   \n",
            "3  Unable to answer with confidence based on the ...             is_blank   \n",
            "4                                               TRUE                    1   \n",
            "\n",
            "  answer_unit                       ref_id  \\\n",
            "0    is_blank                ['chung2025']   \n",
            "1       tCO2e            ['patterson2021']   \n",
            "2          GB                 ['chen2024']   \n",
            "3         MWh                     is_blank   \n",
            "4    is_blank  ['wu2021b','patterson2021']   \n",
            "\n",
            "                                             ref_url  \\\n",
            "0               ['https://arxiv.org/pdf/2505.06371']   \n",
            "1               ['https://arxiv.org/pdf/2104.10350']   \n",
            "2               ['https://arxiv.org/pdf/2405.01814']   \n",
            "3                                           is_blank   \n",
            "4  ['https://arxiv.org/abs/2108.06738','https://a...   \n",
            "\n",
            "                                supporting_materials  \\\n",
            "0  We present the ML.ENERGY Benchmark, a benchmar...   \n",
            "1  \"Training GShard-600B used 24 MWh and produced...   \n",
            "2  Table 3: Large language models used for evalua...   \n",
            "3                                           is_blank   \n",
            "4  Wu 2021, body text near Fig. 1: \"â¦between tr...   \n",
            "\n",
            "                                         explanation  \n",
            "0                                              Quote  \n",
            "1                                              Quote  \n",
            "2                                            Table 3  \n",
            "3                                           is_blank  \n",
            "4  The >40% statement is explicit in Wu. Patterso...  \n",
            "  ï»¿id                                           question  answer  \\\n",
            "0  q001  What was the average increase in U.S. data cen...     NaN   \n",
            "1  q002  In 2023, what was the estimated amount of cars...     NaN   \n",
            "2  q004  How many data centers did AWS begin using recy...     NaN   \n",
            "3  q005  Since NVIDIA doesn't release the embodied carb...     NaN   \n",
            "4  q006  By what factor was the estimated amortized tra...     NaN   \n",
            "\n",
            "   answer_value   answer_unit  ref_id  ref_url  supporting_materials  \\\n",
            "0           NaN       percent     NaN      NaN                   NaN   \n",
            "1           NaN          cars     NaN      NaN                   NaN   \n",
            "2           NaN  data centers     NaN      NaN                   NaN   \n",
            "3           NaN        kg/GPU     NaN      NaN                   NaN   \n",
            "4           NaN         ratio     NaN      NaN                   NaN   \n",
            "\n",
            "   explanation  \n",
            "0          NaN  \n",
            "1          NaN  \n",
            "2          NaN  \n",
            "3          NaN  \n",
            "4          NaN  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pdfplumber\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-13T07:16:29.33218Z",
          "iopub.execute_input": "2025-11-13T07:16:29.332565Z",
          "iopub.status.idle": "2025-11-13T07:16:29.347689Z",
          "shell.execute_reply.started": "2025-11-13T07:16:29.332513Z",
          "shell.execute_reply": "2025-11-13T07:16:29.345839Z"
        },
        "id": "-rja2CkkEWwC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUrX21w4mzCp",
        "outputId": "e49a047f-2155-4bda-fdcf-102041cc1895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content\"\n",
        "pdf_dir = os.path.join(BASE_DIR, \"papers\")\n",
        "text_dir = os.path.join(BASE_DIR, \"processed_texts\")\n",
        "\n",
        "os.makedirs(pdf_dir, exist_ok=True)\n",
        "os.makedirs(text_dir, exist_ok=True)\n",
        "\n",
        "print(\"PDF 下載目錄:\", pdf_dir)\n",
        "print(\"文字輸出目錄:\", text_dir)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-13T07:16:29.348805Z",
          "iopub.status.idle": "2025-11-13T07:16:29.349234Z",
          "shell.execute_reply.started": "2025-11-13T07:16:29.349021Z",
          "shell.execute_reply": "2025-11-13T07:16:29.349038Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoIHTu-TEWwD",
        "outputId": "39c0e3d5-98de-4cc5-cf87-c4f6fc6df865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF 下載目錄: /content/papers\n",
            "文字輸出目錄: /content/processed_texts\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"使用 PyMuPDF 提取文字（保留空格）\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page in doc:\n",
        "            text += page.get_text(\"text\") + \"\\n\"\n",
        "        doc.close()\n",
        "\n",
        "        if not text.strip():\n",
        "            print(f\"⚠️ Using OCR for {pdf_path} ...\")\n",
        "            # OCR 備用方案\n",
        "            from pdf2image import convert_from_path\n",
        "            import pytesseract\n",
        "            pages = convert_from_path(pdf_path, dpi=300)\n",
        "            for page in pages:\n",
        "                text += pytesseract.image_to_string(page, lang=\"eng\") + \"\\n\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-13T07:16:29.351252Z",
          "iopub.status.idle": "2025-11-13T07:16:29.351692Z",
          "shell.execute_reply.started": "2025-11-13T07:16:29.351472Z",
          "shell.execute_reply": "2025-11-13T07:16:29.351489Z"
        },
        "id": "5apoP5HsEWwD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdf(url, doc_id):\n",
        "    pdf_path = os.path.join(pdf_dir, f\"{doc_id}.pdf\")\n",
        "    if os.path.exists(pdf_path):\n",
        "        return pdf_path\n",
        "\n",
        "    try:\n",
        "        print(f\"Downloading {doc_id} ...\")\n",
        "        response = requests.get(url, timeout=30)\n",
        "        if response.status_code == 200 and \"pdf\" in response.headers.get(\"Content-Type\", \"\"):\n",
        "            with open(pdf_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Saved {pdf_path}\")\n",
        "            return pdf_path\n",
        "        else:\n",
        "            print(f\"⚠️ Failed {doc_id}: status {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {doc_id}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-13T07:16:29.353427Z",
          "iopub.status.idle": "2025-11-13T07:16:29.353746Z",
          "shell.execute_reply.started": "2025-11-13T07:16:29.353615Z",
          "shell.execute_reply": "2025-11-13T07:16:29.353627Z"
        },
        "id": "CLDadGfNEWwE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import warnings\n",
        "import logging\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# 抑制警告\n",
        "logging.getLogger('pdfminer').setLevel(logging.ERROR)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def download_pdf(url, doc_id, papers_dir=\"papers\"):\n",
        "    \"\"\"下載 PDF 文件\"\"\"\n",
        "    os.makedirs(papers_dir, exist_ok=True)\n",
        "    pdf_path = os.path.join(papers_dir, f\"{doc_id}.pdf\")\n",
        "\n",
        "    if os.path.exists(pdf_path):\n",
        "        return pdf_path\n",
        "\n",
        "    print(f\"⬇️ Downloading {doc_id} ...\")\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        if response.status_code == 200 and response.content[:4] == b'%PDF':\n",
        "            with open(pdf_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"✅ Saved {pdf_path}\")\n",
        "            return pdf_path\n",
        "        else:\n",
        "            print(f\"⚠️ Failed {doc_id}: status {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed {doc_id}: {type(e).__name__}\")\n",
        "    return None\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"提取 PDF 文字\"\"\"\n",
        "    try:\n",
        "        return extract_text(pdf_path)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# ===== 步驟 1: 修正 zschache2025 的 URL =====\n",
        "print(\"🔧 更新 zschache2025 的 URL...\")\n",
        "metadata.loc[metadata['id'] == 'zschache2025', 'url'] = 'https://arxiv.org/pdf/2508.14170'\n",
        "print(\"✅ URL 已更新\\n\")\n",
        "\n",
        "# ===== 步驟 2: 主循環處理所有文件 =====\n",
        "text_dir = \"processed_texts\"\n",
        "os.makedirs(text_dir, exist_ok=True)\n",
        "\n",
        "success_count = 0\n",
        "fail_count = 0\n",
        "failed_ids = []\n",
        "\n",
        "for i, row in metadata.iterrows():\n",
        "    doc_id = row[\"id\"]\n",
        "    url = row[\"url\"]\n",
        "\n",
        "    # 下載 PDF\n",
        "    pdf_path = download_pdf(url, doc_id)\n",
        "\n",
        "    # 如果下載成功,提取文字\n",
        "    if pdf_path:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        if text.strip():\n",
        "            out_path = os.path.join(text_dir, f\"{doc_id}.txt\")\n",
        "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(text)\n",
        "            print(f\"📝 Saved text -> {out_path}\\n\")\n",
        "            success_count += 1\n",
        "        else:\n",
        "            print(f\"⚠️ {doc_id}: No text extracted\\n\")\n",
        "            fail_count += 1\n",
        "            failed_ids.append(doc_id)\n",
        "    else:\n",
        "        fail_count += 1\n",
        "        failed_ids.append(doc_id)\n",
        "\n",
        "# ===== 步驟 3: 顯示最終統計結果 =====\n",
        "print(\"=\"*70)\n",
        "print(\"🎉 所有處理完成!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"✅ 成功: {success_count}/{len(metadata)} 篇\")\n",
        "print(f\"❌ 失敗: {fail_count}/{len(metadata)} 篇\")\n",
        "print(f\"📊 成功率: {success_count/len(metadata)*100:.1f}%\")\n",
        "\n",
        "if failed_ids:\n",
        "    print(f\"\\n失敗的文件 ID:\")\n",
        "    for doc_id in failed_ids:\n",
        "        print(f\"  • {doc_id}\")\n",
        "else:\n",
        "    print(f\"\\n🌟 完美!所有 {len(metadata)} 篇論文都已成功處理!\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ===== 步驟 4: 驗證文字檔數量 =====\n",
        "text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n",
        "print(f\"\\n📁 已產生的文字檔數量: {len(text_files)}\")\n",
        "\n",
        "# 特別驗證 zschache2025 是否成功\n",
        "if \"zschache2025.txt\" in text_files:\n",
        "    print(f\"✅ zschache2025.txt 已成功產生!\")\n",
        "else:\n",
        "    print(f\"⚠️ zschache2025.txt 尚未產生\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtJveYspZr2E",
        "outputId": "56d056c8-0b45-4bfe-a7ab-7d8276e97d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 更新 zschache2025 的 URL...\n",
            "✅ URL 已更新\n",
            "\n",
            "⬇️ Downloading amazon2023 ...\n",
            "✅ Saved papers/amazon2023.pdf\n",
            "📝 Saved text -> processed_texts/amazon2023.txt\n",
            "\n",
            "⬇️ Downloading chen2024 ...\n",
            "✅ Saved papers/chen2024.pdf\n",
            "📝 Saved text -> processed_texts/chen2024.txt\n",
            "\n",
            "⬇️ Downloading chung2025 ...\n",
            "✅ Saved papers/chung2025.pdf\n",
            "📝 Saved text -> processed_texts/chung2025.txt\n",
            "\n",
            "⬇️ Downloading cottier2024 ...\n",
            "✅ Saved papers/cottier2024.pdf\n",
            "📝 Saved text -> processed_texts/cottier2024.txt\n",
            "\n",
            "⬇️ Downloading dodge2022 ...\n",
            "✅ Saved papers/dodge2022.pdf\n",
            "📝 Saved text -> processed_texts/dodge2022.txt\n",
            "\n",
            "⬇️ Downloading ebert2024 ...\n",
            "✅ Saved papers/ebert2024.pdf\n",
            "📝 Saved text -> processed_texts/ebert2024.txt\n",
            "\n",
            "⬇️ Downloading erben2023 ...\n",
            "✅ Saved papers/erben2023.pdf\n",
            "📝 Saved text -> processed_texts/erben2023.txt\n",
            "\n",
            "⬇️ Downloading fernandez2025 ...\n",
            "✅ Saved papers/fernandez2025.pdf\n",
            "📝 Saved text -> processed_texts/fernandez2025.txt\n",
            "\n",
            "⬇️ Downloading griggs2024 ...\n",
            "✅ Saved papers/griggs2024.pdf\n",
            "📝 Saved text -> processed_texts/griggs2024.txt\n",
            "\n",
            "⬇️ Downloading han2024 ...\n",
            "✅ Saved papers/han2024.pdf\n",
            "📝 Saved text -> processed_texts/han2024.txt\n",
            "\n",
            "⬇️ Downloading jegham2025 ...\n",
            "✅ Saved papers/jegham2025.pdf\n",
            "📝 Saved text -> processed_texts/jegham2025.txt\n",
            "\n",
            "⬇️ Downloading khan2025 ...\n",
            "✅ Saved papers/khan2025.pdf\n",
            "📝 Saved text -> processed_texts/khan2025.txt\n",
            "\n",
            "⬇️ Downloading kim2025 ...\n",
            "✅ Saved papers/kim2025.pdf\n",
            "📝 Saved text -> processed_texts/kim2025.txt\n",
            "\n",
            "⬇️ Downloading li2025a ...\n",
            "✅ Saved papers/li2025a.pdf\n",
            "📝 Saved text -> processed_texts/li2025a.txt\n",
            "\n",
            "⬇️ Downloading li2025b ...\n",
            "✅ Saved papers/li2025b.pdf\n",
            "📝 Saved text -> processed_texts/li2025b.txt\n",
            "\n",
            "⬇️ Downloading luccioni2023 ...\n",
            "✅ Saved papers/luccioni2023.pdf\n",
            "📝 Saved text -> processed_texts/luccioni2023.txt\n",
            "\n",
            "⬇️ Downloading luccioni2024 ...\n",
            "✅ Saved papers/luccioni2024.pdf\n",
            "📝 Saved text -> processed_texts/luccioni2024.txt\n",
            "\n",
            "⬇️ Downloading luccioni2025a ...\n",
            "✅ Saved papers/luccioni2025a.pdf\n",
            "📝 Saved text -> processed_texts/luccioni2025a.txt\n",
            "\n",
            "⬇️ Downloading luccioni2025b ...\n",
            "✅ Saved papers/luccioni2025b.pdf\n",
            "📝 Saved text -> processed_texts/luccioni2025b.txt\n",
            "\n",
            "⬇️ Downloading luccioni2025c ...\n",
            "✅ Saved papers/luccioni2025c.pdf\n",
            "📝 Saved text -> processed_texts/luccioni2025c.txt\n",
            "\n",
            "⬇️ Downloading morrison2025 ...\n",
            "✅ Saved papers/morrison2025.pdf\n",
            "📝 Saved text -> processed_texts/morrison2025.txt\n",
            "\n",
            "⬇️ Downloading patterson2021 ...\n",
            "✅ Saved papers/patterson2021.pdf\n",
            "📝 Saved text -> processed_texts/patterson2021.txt\n",
            "\n",
            "⬇️ Downloading rubei2025 ...\n",
            "✅ Saved papers/rubei2025.pdf\n",
            "📝 Saved text -> processed_texts/rubei2025.txt\n",
            "\n",
            "⬇️ Downloading samsi2024 ...\n",
            "✅ Saved papers/samsi2024.pdf\n",
            "📝 Saved text -> processed_texts/samsi2024.txt\n",
            "\n",
            "⬇️ Downloading schwartz2019 ...\n",
            "✅ Saved papers/schwartz2019.pdf\n",
            "📝 Saved text -> processed_texts/schwartz2019.txt\n",
            "\n",
            "⬇️ Downloading shen2024 ...\n",
            "✅ Saved papers/shen2024.pdf\n",
            "📝 Saved text -> processed_texts/shen2024.txt\n",
            "\n",
            "⬇️ Downloading stone2022 ...\n",
            "✅ Saved papers/stone2022.pdf\n",
            "📝 Saved text -> processed_texts/stone2022.txt\n",
            "\n",
            "⬇️ Downloading strubell2019 ...\n",
            "✅ Saved papers/strubell2019.pdf\n",
            "📝 Saved text -> processed_texts/strubell2019.txt\n",
            "\n",
            "⬇️ Downloading wu2021a ...\n",
            "✅ Saved papers/wu2021a.pdf\n",
            "📝 Saved text -> processed_texts/wu2021a.txt\n",
            "\n",
            "⬇️ Downloading wu2021b ...\n",
            "✅ Saved papers/wu2021b.pdf\n",
            "📝 Saved text -> processed_texts/wu2021b.txt\n",
            "\n",
            "⬇️ Downloading xia2024 ...\n",
            "✅ Saved papers/xia2024.pdf\n",
            "📝 Saved text -> processed_texts/xia2024.txt\n",
            "\n",
            "⬇️ Downloading zschache2025 ...\n",
            "✅ Saved papers/zschache2025.pdf\n",
            "📝 Saved text -> processed_texts/zschache2025.txt\n",
            "\n",
            "======================================================================\n",
            "🎉 所有處理完成!\n",
            "======================================================================\n",
            "✅ 成功: 32/32 篇\n",
            "❌ 失敗: 0/32 篇\n",
            "📊 成功率: 100.0%\n",
            "\n",
            "🌟 完美!所有 32 篇論文都已成功處理!\n",
            "======================================================================\n",
            "\n",
            "📁 已產生的文字檔數量: 32\n",
            "✅ zschache2025.txt 已成功產生!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Corpus Side**"
      ],
      "metadata": {
        "id": "GRL1yvfsgaiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Environment Setup**"
      ],
      "metadata": {
        "id": "l4PJzqa6gr9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys\n",
        "\n",
        "packages = [\n",
        "    \"pdfplumber\",\n",
        "    \"sentence-transformers\",\n",
        "    \"faiss-cpu\",\n",
        "    \"tqdm\",\n",
        "    \"fuzzywuzzy\",\n",
        "    \"python-Levenshtein\",\n",
        "    \"rank-bm25\",\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n"
      ],
      "metadata": {
        "id": "1hGgsIfMhB3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Module Import**"
      ],
      "metadata": {
        "id": "sFwQinONhCcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import ast\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import pdfplumber\n",
        "import faiss\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from fuzzywuzzy import fuzz\n",
        "from difflib import SequenceMatcher\n",
        "from rank_bm25 import BM25Okapi\n"
      ],
      "metadata": {
        "id": "O8Ih_ABbhEYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Dataset Loading**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PrkGkbkHhGYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIR = Path(\"/root/.cache/kagglehub/competitions/WattBot2025\")\n",
        "\n",
        "WORK_DIR = Path(\"/content/wattbot_working\")\n",
        "WORK_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "pdf_dir   = WORK_DIR / \"pdfs\"\n",
        "text_dir  = WORK_DIR / \"texts\"\n",
        "chunk_dir = WORK_DIR / \"chunks\"\n",
        "\n",
        "for d in [pdf_dir, text_dir, chunk_dir]:\n",
        "    d.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "metadata = pd.read_csv(INPUT_DIR / \"metadata.csv\", encoding=\"latin1\")\n",
        "train_qa = pd.read_csv(INPUT_DIR / \"train_QA.csv\", encoding=\"latin1\")\n",
        "test_q   = pd.read_csv(INPUT_DIR / \"test_Q.csv\",  encoding=\"latin1\")\n",
        "\n",
        "print(\"metadata:\", metadata.shape)\n",
        "print(\"train_qa:\", train_qa.shape)\n",
        "print(\"test_q:\",   test_q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhnM2LfShIhn",
        "outputId": "54fa284c-a77a-451c-d2e6-b0c750174121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadata: (32, 6)\n",
            "train_qa: (41, 9)\n",
            "test_q: (282, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.4 Text Cleaning**"
      ],
      "metadata": {
        "id": "4TpfO6raN2kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "完整處理流程 - 所有 32 篇文檔\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# 確認路徑\n",
        "text_dir = Path(\"/content/processed_texts\")\n",
        "chunk_dir = Path(\"/content/wattbot_working/chunks\")\n",
        "chunk_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "def split_long_paragraph(text: str, max_words: int = 300) -> list:\n",
        "    \"\"\"將超長段落按句子切分\"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current = []\n",
        "    current_words = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_words = len(sent.split())\n",
        "        if current_words + sent_words > max_words and current:\n",
        "            chunks.append(' '.join(current))\n",
        "            current = [sent]\n",
        "            current_words = sent_words\n",
        "        else:\n",
        "            current.append(sent)\n",
        "            current_words += sent_words\n",
        "\n",
        "    if current:\n",
        "        chunks.append(' '.join(current))\n",
        "    return chunks\n",
        "\n",
        "def clean_and_chunk_document(text_file: Path, doc_id: str, doc_type: str, chunk_words: int = 200):\n",
        "    \"\"\"處理單一文檔\"\"\"\n",
        "    try:\n",
        "        with open(text_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "    if not text.strip():\n",
        "        return []\n",
        "\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = text.replace(\"ﬁ\", \"fi\").replace(\"ﬂ\", \"fl\")\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "    if doc_type == \"paper\":\n",
        "        ref_patterns = [\n",
        "            r\"\\n\\s*References\\s*\\n\",\n",
        "            r\"\\n\\s*REFERENCES\\s*\\n\",\n",
        "            r\"\\n\\s*Bibliography\\s*\\n\",\n",
        "            r\"\\n\\s*BIBLIOGRAPHY\\s*\\n\",\n",
        "            r\"\\nReferences\\s*\\[\",\n",
        "        ]\n",
        "        for pattern in ref_patterns:\n",
        "            match = re.search(pattern, text, flags=re.IGNORECASE)\n",
        "            if match:\n",
        "                text = text[:match.start()]\n",
        "                print(f\"✂️ Removed references from {doc_id}\")\n",
        "                break\n",
        "\n",
        "    text = re.sub(r\"(\\w)-\\s*\\n\\s*(\\w)\", r\"\\1\\2\", text)\n",
        "    text = re.sub(r\"(?<!\\n)(?<=[a-zA-Z0-9])\\n(?=[a-zA-Z0-9])\", \" \", text)\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "    text = re.sub(r',([A-Z])', r', \\1', text)\n",
        "    text = re.sub(r'([a-z0-9])\\.([A-Z])', r'\\1. \\2', text)\n",
        "    text = re.sub(r'(\\])([A-Z])', r'\\1 \\2', text)\n",
        "    text = re.sub(r'([A-Z][a-z]+)(\\d)', r'\\1 \\2', text)\n",
        "    text = re.sub(r'(\\d)([A-Z][a-z])', r'\\1 \\2', text)\n",
        "\n",
        "    text = re.sub(r\"(?m)^\\s*\\d+\\s*$\", \"\", text)\n",
        "    text = re.sub(r\"(?m)^\\s*Page\\s+\\d+.*$\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    if doc_type == \"report\":\n",
        "        text = re.sub(r\"(?mi)^.*?(sustainability|amazon\\.com).*$\", \"\", text)\n",
        "\n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "    cleaned_pars = []\n",
        "    noise_kw = [\"creative commons\", \"cc by\", \"arxiv preprint\", \"copyright\",\n",
        "                \"all rights reserved\", \"licensed\", \"preprint\", \"arxiv:\"]\n",
        "\n",
        "    for p in paragraphs:\n",
        "        p = p.strip()\n",
        "        if not p or len(p.split()) < 5:\n",
        "            continue\n",
        "        if any(kw in p.lower() for kw in noise_kw):\n",
        "            continue\n",
        "        cleaned_pars.append(p)\n",
        "\n",
        "    if not cleaned_pars:\n",
        "        return []\n",
        "\n",
        "    text = \"\\n\\n\".join(cleaned_pars)\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    text = text.strip()\n",
        "\n",
        "    paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "    chunks = []\n",
        "    current = []\n",
        "    current_words = 0\n",
        "\n",
        "    for para in paragraphs:\n",
        "        words = len(para.split())\n",
        "\n",
        "        if words > chunk_words * 1.3:\n",
        "            if current:\n",
        "                chunks.append(\"\\n\\n\".join(current))\n",
        "                current = []\n",
        "                current_words = 0\n",
        "            sub_chunks = split_long_paragraph(para, max_words=chunk_words)\n",
        "            chunks.extend(sub_chunks)\n",
        "            continue\n",
        "\n",
        "        if current_words + words > chunk_words and current_words >= 80:\n",
        "            chunks.append(\"\\n\\n\".join(current))\n",
        "            current = [para]\n",
        "            current_words = words\n",
        "        else:\n",
        "            current.append(para)\n",
        "            current_words += words\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\"\\n\\n\".join(current))\n",
        "\n",
        "    result = []\n",
        "    for i, chunk_text in enumerate(chunks):\n",
        "        if len(chunk_text.split()) >= 30:\n",
        "            result.append({\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_id\": i,\n",
        "                \"type\": \"text\",\n",
        "                \"content\": chunk_text,\n",
        "                \"word_count\": len(chunk_text.split())\n",
        "            })\n",
        "\n",
        "    return result\n",
        "\n",
        "# 執行完整處理\n",
        "print(\"🚀 開始處理全部 32 篇文檔...\\n\")\n",
        "\n",
        "all_chunks = []\n",
        "stats = {\"success\": 0, \"failed\": []}\n",
        "\n",
        "for _, row in metadata.iterrows():\n",
        "    doc_id = row['id']\n",
        "    doc_type = row['type']\n",
        "    text_file = text_dir / f\"{doc_id}.txt\"\n",
        "\n",
        "    if not text_file.exists():\n",
        "        print(f\"⚠️  {doc_id}.txt 不存在\")\n",
        "        stats[\"failed\"].append(doc_id)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        chunks = clean_and_chunk_document(text_file, doc_id, doc_type, chunk_words=200)\n",
        "\n",
        "        if chunks:\n",
        "            all_chunks.extend(chunks)\n",
        "            stats[\"success\"] += 1\n",
        "            avg = sum(c['word_count'] for c in chunks) / len(chunks)\n",
        "            print(f\"✓ {doc_id:20s} | {len(chunks):3d} chunks | avg {avg:.0f} words\")\n",
        "        else:\n",
        "            print(f\"⚠️  {doc_id} 無內容\")\n",
        "            stats[\"failed\"].append(doc_id)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {doc_id} 失敗: {e}\")\n",
        "        stats[\"failed\"].append(doc_id)\n",
        "\n",
        "# 儲存\n",
        "output_file = chunk_dir / \"chunks_all.jsonl\"\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for chunk in all_chunks:\n",
        "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# 統計\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"✅ 完成！成功: {stats['success']}/32\")\n",
        "print(f\"📊 總 chunks: {len(all_chunks)}\")\n",
        "if all_chunks:\n",
        "    avg = sum(c['word_count'] for c in all_chunks) / len(all_chunks)\n",
        "    print(f\"📏 平均大小: {avg:.1f} words\")\n",
        "if stats['failed']:\n",
        "    print(f\"❌ 失敗: {stats['failed']}\")\n",
        "print(f\"💾 輸出: {output_file}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if all_chunks:\n",
        "    word_counts = [c['word_count'] for c in all_chunks]\n",
        "    print(f\"\\n📊 字數分布:\")\n",
        "    print(f\"  最小: {min(word_counts)}\")\n",
        "    print(f\"  最大: {max(word_counts)}\")\n",
        "    print(f\"  中位數: {sorted(word_counts)[len(word_counts)//2]}\")\n",
        "\n",
        "    doc_chunks = defaultdict(int)\n",
        "    for c in all_chunks:\n",
        "        doc_chunks[c['doc_id']] += 1\n",
        "\n",
        "    print(f\"\\n📄 每篇文檔的 chunks:\")\n",
        "    print(f\"  平均: {sum(doc_chunks.values())/len(doc_chunks):.1f}\")\n",
        "    print(f\"  範圍: {min(doc_chunks.values())} - {max(doc_chunks.values())}\")\n",
        "\n",
        "print(\"\\n✨ 準備建立檢索系統！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4czCrD4avvt",
        "outputId": "89b27bb3-c279-4770-c39e-54197532a74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 開始處理全部 32 篇文檔...\n",
            "\n",
            "✓ amazon2023           | 363 chunks | avg 167 words\n",
            "✂️ Removed references from chen2024\n",
            "✓ chen2024             |  48 chunks | avg 161 words\n",
            "✂️ Removed references from chung2025\n",
            "✓ chung2025            |  31 chunks | avg 173 words\n",
            "✂️ Removed references from cottier2024\n",
            "✓ cottier2024          |  33 chunks | avg 161 words\n",
            "✂️ Removed references from dodge2022\n",
            "✓ dodge2022            |  47 chunks | avg 184 words\n",
            "✂️ Removed references from ebert2024\n",
            "✓ ebert2024            |  63 chunks | avg 154 words\n",
            "✓ erben2023            |  84 chunks | avg 160 words\n",
            "✂️ Removed references from fernandez2025\n",
            "✓ fernandez2025        |  34 chunks | avg 159 words\n",
            "✂️ Removed references from griggs2024\n",
            "✓ griggs2024           |  37 chunks | avg 159 words\n",
            "✂️ Removed references from han2024\n",
            "✓ han2024              |  76 chunks | avg 159 words\n",
            "✂️ Removed references from jegham2025\n",
            "✓ jegham2025           |  43 chunks | avg 155 words\n",
            "✂️ Removed references from khan2025\n",
            "✓ khan2025             |  22 chunks | avg 166 words\n",
            "✂️ Removed references from kim2025\n",
            "✓ kim2025              |  43 chunks | avg 175 words\n",
            "✂️ Removed references from li2025a\n",
            "✓ li2025a              |  32 chunks | avg 165 words\n",
            "✂️ Removed references from li2025b\n",
            "✓ li2025b              |  30 chunks | avg 156 words\n",
            "✂️ Removed references from luccioni2023\n",
            "✓ luccioni2023         |  37 chunks | avg 183 words\n",
            "✂️ Removed references from luccioni2024\n",
            "✓ luccioni2024         |  42 chunks | avg 182 words\n",
            "✂️ Removed references from luccioni2025a\n",
            "✓ luccioni2025a        |  61 chunks | avg 161 words\n",
            "✂️ Removed references from luccioni2025b\n",
            "✓ luccioni2025b        |  49 chunks | avg 188 words\n",
            "✂️ Removed references from luccioni2025c\n",
            "✓ luccioni2025c        |  27 chunks | avg 177 words\n",
            "✂️ Removed references from morrison2025\n",
            "✓ morrison2025         |  42 chunks | avg 154 words\n",
            "✂️ Removed references from patterson2021\n",
            "✓ patterson2021        |  50 chunks | avg 175 words\n",
            "✂️ Removed references from rubei2025\n",
            "✓ rubei2025            |  28 chunks | avg 158 words\n",
            "✂️ Removed references from samsi2024\n",
            "✓ samsi2024            |  34 chunks | avg 164 words\n",
            "✂️ Removed references from schwartz2019\n",
            "✓ schwartz2019         |  31 chunks | avg 156 words\n",
            "✂️ Removed references from shen2024\n",
            "✓ shen2024             |  23 chunks | avg 183 words\n",
            "✓ stone2022            | 175 chunks | avg 162 words\n",
            "✂️ Removed references from strubell2019\n",
            "✓ strubell2019         |  20 chunks | avg 161 words\n",
            "✂️ Removed references from wu2021a\n",
            "✓ wu2021a              |  52 chunks | avg 156 words\n",
            "✂️ Removed references from wu2021b\n",
            "✓ wu2021b              |  31 chunks | avg 179 words\n",
            "✓ xia2024              |  50 chunks | avg 168 words\n",
            "✂️ Removed references from zschache2025\n",
            "✓ zschache2025         |  40 chunks | avg 173 words\n",
            "\n",
            "======================================================================\n",
            "✅ 完成！成功: 32/32\n",
            "📊 總 chunks: 1778\n",
            "📏 平均大小: 166.0 words\n",
            "💾 輸出: /content/wattbot_working/chunks/chunks_all.jsonl\n",
            "======================================================================\n",
            "\n",
            "📊 字數分布:\n",
            "  最小: 37\n",
            "  最大: 323\n",
            "  中位數: 173\n",
            "\n",
            "📄 每篇文檔的 chunks:\n",
            "  平均: 55.6\n",
            "  範圍: 20 - 363\n",
            "\n",
            "✨ 準備建立檢索系統！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.6 Document Parsing**"
      ],
      "metadata": {
        "id": "vQCk1ydRhPWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymupdf rank-bm25 sentence-transformers faiss-cpu chardet"
      ],
      "metadata": {
        "id": "FIlFokpDxA58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Part 1: 測試 PDF Parsing 能否找到關鍵資訊\n",
        "檢查表格、數字是否能正確提取\n",
        "\"\"\"\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# 讀取資料\n",
        "data_dir = Path(\"/root/.cache/kagglehub/competitions/WattBot2025\")\n",
        "train_qa = pd.read_csv(data_dir / 'train_QA.csv', encoding='utf-8-sig')\n",
        "pdf_dir = Path(\"/content/papers\")\n",
        "\n",
        "# 測試案例\n",
        "test_cases = [\n",
        "    {\n",
        "        'id': 'q054',\n",
        "        'ref_id': 'chen2024',\n",
        "        'answer': '64.7 GB',\n",
        "        'keywords': ['LLaMA-33B', '64.7', 'Table 3'],\n",
        "        'question': train_qa[train_qa['id'] == 'q054']['question'].values[0]\n",
        "    },\n",
        "    {\n",
        "        'id': 'q297',\n",
        "        'ref_id': 'zschache2025',\n",
        "        'answer': '8.720430108',\n",
        "        'keywords': ['Qwen', '7B', '72B', '5.58', '48.66'],\n",
        "        'question': train_qa[train_qa['id'] == 'q297']['question'].values[0]\n",
        "    },\n",
        "    {\n",
        "        'id': 'q009',\n",
        "        'ref_id': 'patterson2021',\n",
        "        'answer': '4.3 tCO2e',\n",
        "        'keywords': ['GShard-600B', '4.3', 'tCO2e'],\n",
        "        'question': train_qa[train_qa['id'] == 'q009']['question'].values[0]\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"🔍 測試：PDF Parsing 能否找到答案的關鍵資訊\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "overall_results = []\n",
        "\n",
        "for test in test_cases:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"📋 題目 {test['id']}: {test['question'][:100]}...\")\n",
        "    print(f\"✓ 期望答案: {test['answer']}\")\n",
        "    print(f\"📄 文件: {test['ref_id']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    pdf_path = pdf_dir / f\"{test['ref_id']}.pdf\"\n",
        "\n",
        "    if not pdf_path.exists():\n",
        "        print(f\"❌ PDF 不存在\")\n",
        "        overall_results.append({'id': test['id'], 'found_ratio': 0})\n",
        "        continue\n",
        "\n",
        "    # 開啟 PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    # 搜尋每個關鍵字\n",
        "    found_info = {}\n",
        "\n",
        "    for kw in test['keywords']:\n",
        "        found = False\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            text = page.get_text()\n",
        "\n",
        "            if kw in text:\n",
        "                found = True\n",
        "                idx = text.find(kw)\n",
        "                # 取前後 150 字元作為上下文\n",
        "                context = text[max(0, idx-150):min(len(text), idx+150)]\n",
        "                found_info[kw] = {\n",
        "                    'page': page_num + 1,\n",
        "                    'context': context.replace('\\n', ' ').strip()\n",
        "                }\n",
        "                break\n",
        "\n",
        "        if found:\n",
        "            print(f\"  ✓ 找到 '{kw}' 在第 {found_info[kw]['page']} 頁\")\n",
        "            print(f\"    上下文: ...{found_info[kw]['context'][:100]}...\")\n",
        "        else:\n",
        "            print(f\"  ✗ 未找到 '{kw}'\")\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "    # 判斷結果\n",
        "    found_count = len(found_info)\n",
        "    total_count = len(test['keywords'])\n",
        "    ratio = found_count / total_count\n",
        "\n",
        "    overall_results.append({\n",
        "        'id': test['id'],\n",
        "        'found_ratio': ratio,\n",
        "        'found': found_count,\n",
        "        'total': total_count\n",
        "    })\n",
        "\n",
        "    if found_count == total_count:\n",
        "        print(f\"\\n  ✅ 所有關鍵字都找到了！({found_count}/{total_count})\")\n",
        "    else:\n",
        "        print(f\"\\n  ⚠️  只找到部分關鍵字 ({found_count}/{total_count})\")\n",
        "\n",
        "# 總結\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"📊 診斷結果總結\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "avg_ratio = sum(r['found_ratio'] for r in overall_results) / len(overall_results)\n",
        "print(f\"\\n平均找到率: {avg_ratio:.1%}\")\n",
        "\n",
        "for r in overall_results:\n",
        "    print(f\"  {r['id']}: {r['found']}/{r['total']} ({r['found_ratio']:.0%})\")\n",
        "\n",
        "print(\"\\n💡 建議:\")\n",
        "if avg_ratio >= 0.8:\n",
        "    print(\"  ✅ Parsing 效果良好，可以繼續使用當前的提取方法\")\n",
        "elif avg_ratio >= 0.5:\n",
        "    print(\"  ⚠️  Parsing 效果一般，建議:\")\n",
        "    print(\"     1. 檢查表格提取（pdfplumber 對表格效果更好）\")\n",
        "    print(\"     2. 加強數字提取\")\n",
        "else:\n",
        "    print(\"  ❌ Parsing 效果不佳，建議:\")\n",
        "    print(\"     1. 使用 pdfplumber 處理表格\")\n",
        "    print(\"     2. 考慮 OCR（如果內容是圖片）\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD52tCL2cFTi",
        "outputId": "af267385-33d2-42b5-fcec-0bf6eacb43f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🔍 測試：PDF Parsing 能否找到答案的關鍵資訊\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "📋 題目 q054: What is the model size in gigabytes (GB) for the LLaMA-33B model?...\n",
            "✓ 期望答案: 64.7 GB\n",
            "📄 文件: chen2024\n",
            "--------------------------------------------------------------------------------\n",
            "  ✓ 找到 'LLaMA-33B' 在第 9 頁\n",
            "    上下文: ...similar outlines and workload charac- teristics and only have minor differences irrelevant to system...\n",
            "  ✓ 找到 '64.7' 在第 9 頁\n",
            "    上下文: ...the same sequence length for evaluation. The summaries of these Table 3: Large language models used ...\n",
            "  ✓ 找到 'Table 3' 在第 9 頁\n",
            "    上下文: ...architectures have similar outlines and workload charac- teristics and only have minor differences i...\n",
            "\n",
            "  ✅ 所有關鍵字都找到了！(3/3)\n",
            "\n",
            "================================================================================\n",
            "📋 題目 q297: When comparing small and large versions of Qwen models in zero-shot classification, how many times m...\n",
            "✓ 期望答案: 8.720430108\n",
            "📄 文件: zschache2025\n",
            "--------------------------------------------------------------------------------\n",
            "  ✓ 找到 'Qwen' 在第 5 頁\n",
            "    上下文: ...Llama 3.1 8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct Llama 3.1 70B https://hugg...\n",
            "  ✓ 找到 '7B' 在第 5 頁\n",
            "    上下文: ...8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct Llama 3.1 70B https://huggingface.co...\n",
            "  ✓ 找到 '72B' 在第 5 頁\n",
            "    上下文: ...struct Llama 3.1 70B https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct Qwen 2.5 7B https...\n",
            "  ✓ 找到 '5.58' 在第 19 頁\n",
            "    上下文: ...47 0.01 67.77 XGBoost Embedding 1 0.21 0.47 2.87 259.94 Llama 3.1 8B 1 5.86 0.35 36.88 572.49 Llama ...\n",
            "  ✓ 找到 '48.66' 在第 19 頁\n",
            "    上下文: ...0.47 2.87 259.94 Llama 3.1 8B 1 5.86 0.35 36.88 572.49 Llama 3.1 70B 2 48.60 0.48 161.59 1082.82 Qwe...\n",
            "\n",
            "  ✅ 所有關鍵字都找到了！(5/5)\n",
            "\n",
            "================================================================================\n",
            "📋 題目 q009: What were the net CO2e emissions from training the GShard-600B model?...\n",
            "✓ 期望答案: 4.3 tCO2e\n",
            "📄 文件: patterson2021\n",
            "--------------------------------------------------------------------------------\n",
            "  ✓ 找到 'GShard-600B' 在第 6 頁\n",
            "    上下文: ...ual  neural  machine  translation  Transformer  model  with  sparsely  gated    mixture-of-experts  ...\n",
            "  ✓ 找到 '4.3' 在第 2 頁\n",
            "    上下文: ...chers  to  measure  energy  usage  and  CO 2 e—or  to  get    a  rough  estimate  using  a  tool  li...\n",
            "  ✓ 找到 'tCO2e' 在第 6 頁\n",
            "    上下文: ....7  96.4  4.8  72.2  552.1  Net  tCO 2 e  for  Model  Training     3.2  46.7  96.4  4.3  59.1  552.1...\n",
            "\n",
            "  ✅ 所有關鍵字都找到了！(3/3)\n",
            "\n",
            "================================================================================\n",
            "📊 診斷結果總結\n",
            "================================================================================\n",
            "\n",
            "平均找到率: 100.0%\n",
            "  q054: 3/3 (100%)\n",
            "  q297: 5/5 (100%)\n",
            "  q009: 3/3 (100%)\n",
            "\n",
            "💡 建議:\n",
            "  ✅ Parsing 效果良好，可以繼續使用當前的提取方法\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.7 Smart Chunking & 結構化提取PDF檔案**"
      ],
      "metadata": {
        "id": "2JpxQN-XhSPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pdfplumber\n",
        "import fitz\n",
        "import unicodedata\n",
        "\n",
        "pdf_dir = Path(\"/content/papers\")\n",
        "chunk_dir = Path(\"/content/chunks\")\n",
        "chunk_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(\"📂 Step 1: 結構化提取 PDF...\")\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = text.replace(\"ﬁ\", \"fi\").replace(\"ﬂ\", \"fl\")\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "    text = re.sub(r\"(\\w)-\\s*\\n\\s*(\\w)\", r\"\\1\\2\", text)\n",
        "    text = re.sub(r\"(?<!\\n)(?<=[a-zA-Z0-9])\\n(?=[a-zA-Z0-9])\", \" \", text)\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "    text = re.sub(r',([A-Z])', r', \\1', text)\n",
        "    text = re.sub(r'([a-z0-9])\\.([A-Z])', r'\\1. \\2', text)\n",
        "    text = re.sub(r'(\\])([A-Z])', r'\\1 \\2', text)\n",
        "    text = re.sub(r'([A-Z][a-z]+)(\\d)', r'\\1 \\2', text)\n",
        "    text = re.sub(r'(\\d)([A-Z][a-z])', r'\\1 \\2', text)\n",
        "    text = re.sub(r'(\\d)([a-z]+)', r'\\1 \\2', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "class StructuredDocumentExtractor:\n",
        "    def extract(self, pdf_path: Path) -> Dict:\n",
        "        doc_id = pdf_path.stem\n",
        "        result = {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"pages\": [],\n",
        "            \"tables\": [],\n",
        "            \"figures\": [],\n",
        "            \"full_text\": \"\",\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            page_texts = []\n",
        "\n",
        "            for page_num in range(len(doc)):\n",
        "                page = doc[page_num]\n",
        "                page_idx = page_num + 1\n",
        "\n",
        "                raw_text = page.get_text(\"text\")\n",
        "                clean = clean_text(raw_text) if raw_text else \"\"\n",
        "\n",
        "                if clean:\n",
        "                    page_texts.append(clean)\n",
        "\n",
        "                    if re.search(r\"Figure\\s+\\d+\", clean, re.IGNORECASE):\n",
        "                        captions = re.findall(\n",
        "                            r\"(Figure\\s+\\d+[:\\.].*?)(?=\\n\\n|\\n[A-Z]|$)\",\n",
        "                            clean, re.IGNORECASE | re.DOTALL\n",
        "                        )\n",
        "                        for cap in captions:\n",
        "                            result[\"figures\"].append({\"page\": page_idx, \"caption\": cap.strip()})\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    page_idx = page_num + 1\n",
        "                    tables = page.extract_tables()\n",
        "\n",
        "                    for table_idx, table in enumerate(tables or []):\n",
        "                        if table and len(table) > 1:\n",
        "                            headers = [str(cell).strip() if cell else \"\" for cell in table[0]]\n",
        "                            rows = []\n",
        "                            for row in table[1:]:\n",
        "                                clean_row = [str(cell).strip() if cell else \"\" for cell in row]\n",
        "                                if any(clean_row):\n",
        "                                    rows.append(clean_row)\n",
        "\n",
        "                            if rows and any(headers):\n",
        "                                table_id = f\"Table_{page_idx}_{table_idx+1}\"\n",
        "                                structured_table = {\n",
        "                                    \"id\": table_id,\n",
        "                                    \"page\": page_idx,\n",
        "                                    \"headers\": headers,\n",
        "                                    \"rows\": rows,\n",
        "                                }\n",
        "                                result[\"tables\"].append(structured_table)\n",
        "\n",
        "            if page_texts:\n",
        "                result[\"full_text\"] = \"\\n\\n\".join(page_texts)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ {doc_id}: {e}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "extractor = StructuredDocumentExtractor()\n",
        "all_docs = {}\n",
        "\n",
        "for pdf_file in tqdm(list(pdf_dir.glob(\"*.pdf\")), desc=\"提取 PDF\"):\n",
        "    doc_data = extractor.extract(pdf_file)\n",
        "    if doc_data[\"full_text\"]:\n",
        "        all_docs[doc_data[\"doc_id\"]] = doc_data\n",
        "\n",
        "print(f\"\\n✅ 提取完成: {len(all_docs)} 文檔, {sum(len(d['tables']) for d in all_docs.values())} 表格\\n\")\n",
        "\n",
        "class HybridChunker:\n",
        "    def __init__(self, text_chunk_words: int = 200):\n",
        "        self.text_chunk_words = text_chunk_words\n",
        "\n",
        "    def chunk_document(self, doc_id: str, doc_data: Dict) -> List[Dict]:\n",
        "        chunks = []\n",
        "\n",
        "        full_text = doc_data.get(\"full_text\", \"\")\n",
        "        if full_text:\n",
        "            text_chunks = self._chunk_text(full_text)\n",
        "            for text in text_chunks:\n",
        "                chunks.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"chunk_id\": len(chunks),\n",
        "                    \"type\": \"text\",\n",
        "                    \"content\": text,\n",
        "                    \"word_count\": len(text.split()),\n",
        "                })\n",
        "\n",
        "        for table in doc_data.get(\"tables\", []):\n",
        "            table_content = self._format_table(table)\n",
        "            if (table_content and len(table_content.split()) > 10 and any(h for h in table.get('headers', []) if h.strip())): #原本是15\n",
        "                chunks.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"chunk_id\": len(chunks),\n",
        "                    \"type\": \"table\",\n",
        "                    \"content\": table_content,\n",
        "                    \"word_count\": len(table_content.split()),\n",
        "                })\n",
        "\n",
        "        for fig in doc_data.get(\"figures\", []):\n",
        "            caption = fig.get(\"caption\", \"\").strip()\n",
        "            if caption and len(caption.split()) >= 5:\n",
        "                chunks.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"chunk_id\": len(chunks),\n",
        "                    \"type\": \"figure\",\n",
        "                    \"content\": f\"Figure: {caption}\",\n",
        "                    \"word_count\": len(caption.split()),\n",
        "                })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _chunk_text(self, text: str) -> List[str]:\n",
        "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "        chunks = []\n",
        "        current = []\n",
        "        current_words = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            words = len(para.split())\n",
        "\n",
        "            if words > self.text_chunk_words * 1.3:\n",
        "                if current:\n",
        "                    chunks.append(\"\\n\\n\".join(current))\n",
        "                    current = []\n",
        "                    current_words = 0\n",
        "\n",
        "                sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "                temp = []\n",
        "                temp_words = 0\n",
        "                for sent in sentences:\n",
        "                    sent_words = len(sent.split())\n",
        "                    if temp_words + sent_words > self.text_chunk_words and temp:\n",
        "                        chunks.append(' '.join(temp))\n",
        "                        temp = [sent]\n",
        "                        temp_words = sent_words\n",
        "                    else:\n",
        "                        temp.append(sent)\n",
        "                        temp_words += sent_words\n",
        "                if temp:\n",
        "                    chunks.append(' '.join(temp))\n",
        "                continue\n",
        "\n",
        "            if current_words + words > self.text_chunk_words and current_words >= 80:\n",
        "                chunks.append(\"\\n\\n\".join(current))\n",
        "                current = [para]\n",
        "                current_words = words\n",
        "            else:\n",
        "                current.append(para)\n",
        "                current_words += words\n",
        "\n",
        "        if current:\n",
        "            chunks.append(\"\\n\\n\".join(current))\n",
        "\n",
        "        return [c for c in chunks if len(c.split()) >= 20] #原本是30! 改更寬鬆一點\n",
        "\n",
        "    def _format_table(self, table: Dict) -> str:\n",
        "        lines = [f\"[{table['id']} on Page {table['page']}]\"]\n",
        "\n",
        "        headers = table.get('headers', [])\n",
        "        rows = table.get('rows', [])\n",
        "\n",
        "        if headers and rows:\n",
        "            clean_headers = [h.strip() for h in headers if h.strip()]\n",
        "\n",
        "            summary_parts = []\n",
        "            for row in rows[:3]:\n",
        "                clean_row = [cell.strip() for cell in row if cell.strip()]\n",
        "                if len(clean_row) >= 2:\n",
        "                    if len(clean_headers) == len(clean_row):\n",
        "                        row_desc = []\n",
        "                        for header, value in zip(clean_headers, clean_row):\n",
        "                            row_desc.append(f\"{value} ({header})\")\n",
        "                        summary_parts.append(\", \".join(row_desc))\n",
        "\n",
        "            if summary_parts:\n",
        "                lines.append(\"Summary: \" + \"; \".join(summary_parts[:2]))\n",
        "\n",
        "        if headers and any(headers):\n",
        "            lines.append(\"\\nStructured Data:\")\n",
        "            clean_headers = [h.strip() for h in headers if h.strip()]\n",
        "            lines.append(\" | \".join(clean_headers))\n",
        "\n",
        "        if rows:\n",
        "            for row in rows:\n",
        "                if any(row):\n",
        "                    clean_row = [cell.strip() for cell in row if cell.strip()]\n",
        "                    if clean_row:\n",
        "                        lines.append(\" | \".join(clean_row))\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "chunker = HybridChunker(text_chunk_words=200)\n",
        "all_chunks = []\n",
        "\n",
        "for doc_id, doc_data in tqdm(all_docs.items(), desc=\"分塊\"):\n",
        "    chunks = chunker.chunk_document(doc_id, doc_data)\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "output_file = Path(\"/content/hybrid_chunks.json\")\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "from collections import Counter\n",
        "type_counts = Counter(c['type'] for c in all_chunks)\n",
        "print(f\"\\n✅ 完成: {len(all_chunks)} chunks\")\n",
        "for t, c in type_counts.items():\n",
        "    print(f\"  {t}: {c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfq7njfKdBxf",
        "outputId": "d6ed5cae-9c32-485c-a767-f2c404b51b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Step 1: 結構化提取 PDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "提取 PDF: 100%|██████████| 32/32 [02:43<00:00,  5.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 提取完成: 32 文檔, 145 表格\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "分塊: 100%|██████████| 32/32 [00:00<00:00, 172.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 完成: 2608 chunks\n",
            "  text: 2372\n",
            "  table: 134\n",
            "  figure: 102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.8 BM25重點測試**"
      ],
      "metadata": {
        "id": "TcbRnpS-fOsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List, Dict\n",
        "import re\n",
        "\n",
        "print(\"📂 載入 chunks...\\n\")\n",
        "\n",
        "text_chunks_file = Path(\"/content/wattbot_working/chunks/chunks_all.jsonl\")\n",
        "text_chunks = []\n",
        "with open(text_chunks_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        text_chunks.append(json.loads(line))\n",
        "print(f\"✓ 純文字 chunks: {len(text_chunks)}\")\n",
        "\n",
        "hybrid_chunks_file = Path(\"/content/hybrid_chunks.json\")\n",
        "with open(hybrid_chunks_file, 'r', encoding='utf-8') as f:\n",
        "    hybrid_chunks = json.load(f)\n",
        "print(f\"✓ 混合 chunks: {len(hybrid_chunks)}\")\n",
        "\n",
        "print(\"\\n🔨 建立雙重 BM25 索引...\\n\")\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    tokens = re.findall(r'\\d+\\.?\\d*%?|\\w+', text)\n",
        "    return tokens\n",
        "\n",
        "text_corpus = [c['content'] for c in text_chunks]\n",
        "text_tokenized = [tokenize(t) for t in text_corpus]\n",
        "bm25_text = BM25Okapi(text_tokenized)\n",
        "print(\"✓ 純文字 BM25 索引完成\")\n",
        "\n",
        "table_chunks = [c for c in hybrid_chunks if c['type'] == 'table']\n",
        "table_corpus = [c['content'] for c in table_chunks]\n",
        "table_tokenized = [tokenize(t) for t in table_corpus]\n",
        "bm25_table = BM25Okapi(table_tokenized)\n",
        "print(f\"✓ 表格 BM25 索引完成 ({len(table_chunks)} 個表格)\\n\")\n",
        "\n",
        "def is_table_query(query: str) -> bool:\n",
        "    table_keywords = [\n",
        "        'table', 'how many', 'how much', 'number', 'size', 'gigabytes',\n",
        "        'gb', 'mb', 'tb', 'watts', 'kwh', 'carbon', 'co2', 'emissions',\n",
        "        'cost', 'dollar', 'percentage', '%', 'comparing', 'compare',\n",
        "        'versus', 'vs', 'difference between', 'times more', 'times less'\n",
        "    ]\n",
        "    query_lower = query.lower()\n",
        "    return any(kw in query_lower for kw in table_keywords)\n",
        "\n",
        "def hybrid_search(query: str, top_k: int = 10) -> List[Dict]:\n",
        "    tokenized_query = tokenize(query)\n",
        "    text_scores = bm25_text.get_scores(tokenized_query)\n",
        "\n",
        "    if is_table_query(query):\n",
        "        table_scores = bm25_table.get_scores(tokenized_query)\n",
        "        text_top_indices = np.argsort(text_scores)[-7:][::-1]\n",
        "        table_top_indices = np.argsort(table_scores)[-3:][::-1]\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for idx in text_top_indices:\n",
        "            results.append({\n",
        "                'rank': len(results) + 1,\n",
        "                'score': float(text_scores[idx]),\n",
        "                'doc_id': text_chunks[idx]['doc_id'],\n",
        "                'type': 'text',\n",
        "                'content': text_chunks[idx]['content'],\n",
        "                'source': 'text_index'\n",
        "            })\n",
        "\n",
        "        for idx in table_top_indices:\n",
        "            results.append({\n",
        "                'rank': len(results) + 1,\n",
        "                'score': float(table_scores[idx]) * 0.8,\n",
        "                'doc_id': table_chunks[idx]['doc_id'],\n",
        "                'type': 'table',\n",
        "                'content': table_chunks[idx]['content'],\n",
        "                'source': 'table_index'\n",
        "            })\n",
        "\n",
        "        results.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        for i, r in enumerate(results):\n",
        "            r['rank'] = i + 1\n",
        "\n",
        "        return results[:top_k]\n",
        "\n",
        "    else:\n",
        "        top_indices = np.argsort(text_scores)[-top_k:][::-1]\n",
        "        results = []\n",
        "        for rank, idx in enumerate(top_indices, 1):\n",
        "            results.append({\n",
        "                'rank': rank,\n",
        "                'score': float(text_scores[idx]),\n",
        "                'doc_id': text_chunks[idx]['doc_id'],\n",
        "                'type': 'text',\n",
        "                'content': text_chunks[idx]['content'],\n",
        "                'source': 'text_index'\n",
        "            })\n",
        "        return results\n",
        "\n",
        "def get_doc_from_chunks(chunks_results: List[Dict]) -> List[str]:\n",
        "    seen = set()\n",
        "    docs = []\n",
        "    for r in chunks_results:\n",
        "        doc_id = r['doc_id']\n",
        "        if doc_id not in seen:\n",
        "            seen.add(doc_id)\n",
        "            docs.append(doc_id)\n",
        "    return docs\n",
        "\n",
        "print(\"📊 評估混合檢索策略...\\n\")\n",
        "\n",
        "data_dir = Path(\"/root/.cache/kagglehub/competitions/WattBot2025\")\n",
        "train_qa = pd.read_csv(data_dir / 'train_QA.csv', encoding='utf-8-sig')\n",
        "\n",
        "def evaluate_hybrid(qa_df: pd.DataFrame):\n",
        "    results = {k: [] for k in [1, 3, 5, 10]}\n",
        "    mrr_scores = []\n",
        "    valid_count = 0\n",
        "    table_query_count = 0\n",
        "\n",
        "    for idx, row in qa_df.iterrows():\n",
        "        question = row['question']\n",
        "        ref_id_raw = row['ref_id']\n",
        "\n",
        "        if pd.isna(ref_id_raw) or str(ref_id_raw).strip() == '':\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if isinstance(ref_id_raw, str):\n",
        "                if ref_id_raw.startswith('['):\n",
        "                    import ast\n",
        "                    true_docs = ast.literal_eval(ref_id_raw)\n",
        "                    if not isinstance(true_docs, list):\n",
        "                        true_docs = [true_docs]\n",
        "                else:\n",
        "                    true_docs = [ref_id_raw.strip()]\n",
        "            else:\n",
        "                true_docs = [str(ref_id_raw).strip()]\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        valid_count += 1\n",
        "\n",
        "        if is_table_query(question):\n",
        "            table_query_count += 1\n",
        "\n",
        "        search_results = hybrid_search(question, top_k=10)\n",
        "        retrieved_docs = get_doc_from_chunks(search_results)\n",
        "\n",
        "        for k in [1, 3, 5, 10]:\n",
        "            top_k_docs = retrieved_docs[:k]\n",
        "            hit = any(doc in top_k_docs for doc in true_docs)\n",
        "            results[k].append(1 if hit else 0)\n",
        "\n",
        "        reciprocal_rank = 0\n",
        "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
        "            if doc_id in true_docs:\n",
        "                reciprocal_rank = 1.0 / rank\n",
        "                break\n",
        "        mrr_scores.append(reciprocal_rank)\n",
        "\n",
        "        if valid_count % 10 == 0:\n",
        "            print(f\"  處理: {valid_count} 問題...\", end='\\r')\n",
        "\n",
        "    print(f\"  ✓ 完成 {valid_count} 問題\" + \" \"*20)\n",
        "\n",
        "    metrics = {}\n",
        "    for k in [1, 3, 5, 10]:\n",
        "        metrics[f'Recall@{k}'] = np.mean(results[k])\n",
        "    metrics['MRR'] = np.mean(mrr_scores)\n",
        "    metrics['total_questions'] = valid_count\n",
        "    metrics['table_queries'] = table_query_count\n",
        "\n",
        "    return metrics\n",
        "\n",
        "metrics = evaluate_hybrid(train_qa)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📈 混合檢索效果（智能路由）\")\n",
        "print(\"=\"*70)\n",
        "print(f\"評估問題數: {metrics['total_questions']}\")\n",
        "print(f\"表格查詢數: {metrics['table_queries']} ({metrics['table_queries']/metrics['total_questions']*100:.1f}%)\")\n",
        "print()\n",
        "for k in [1, 3, 5, 10]:\n",
        "    recall = metrics[f'Recall@{k}']\n",
        "    bar = int(recall * 50)\n",
        "    print(f\"  Recall@{k:2d}  : {recall:6.2%}  {'█' * bar}\")\n",
        "\n",
        "print(f\"\\n  MRR        : {metrics['MRR']:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n🔍 測試關鍵案例...\\n\")\n",
        "\n",
        "test_cases = [\n",
        "    ('q054', 'chen2024', 'What is the model size in gigabytes (GB) for the LLaMA-33B model?'),\n",
        "    ('q297', 'zschache2025', 'When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy does the large model consume?'),\n",
        "    ('q009', 'patterson2021', 'What were the net CO2e emissions from training the GShard-600B model?'),\n",
        "]\n",
        "\n",
        "for qid, expected_doc, question in test_cases:\n",
        "    print(f\"{'─'*70}\")\n",
        "    print(f\"📋 {qid}: {question[:80]}...\")\n",
        "    print(f\"✓ 期望: {expected_doc}\")\n",
        "\n",
        "    is_table = is_table_query(question)\n",
        "    print(f\"🔍 查詢類型: {'表格查詢 📊' if is_table else '文字查詢 📄'}\\n\")\n",
        "\n",
        "    results = hybrid_search(question, top_k=5)\n",
        "\n",
        "    print(\"檢索結果:\")\n",
        "    for r in results:\n",
        "        marker = \"✅\" if r['doc_id'] == expected_doc else \"  \"\n",
        "        icon = \"📊\" if r['type'] == 'table' else \"📄\"\n",
        "        src = r['source']\n",
        "        print(f\"  {marker} #{r['rank']} | {icon} {r['type']:6s} | {src:12s} | {r['score']:6.2f} | {r['doc_id']}\")\n",
        "\n",
        "        if r['doc_id'] == expected_doc and r['rank'] <= 3:\n",
        "            print(f\"       {r['content'][:120]}...\")\n",
        "\n",
        "    retrieved_docs = get_doc_from_chunks(results)\n",
        "    if retrieved_docs[0] == expected_doc:\n",
        "        print(f\"\\n  ✅ 成功！\")\n",
        "    else:\n",
        "        print(f\"\\n  ❌ 失敗\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"📊 三種方法對比\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n方法 1 - 純文字 chunks:\")\n",
        "print(\"  Recall@1: 85.37%  |  Recall@5: 92.68%  |  MRR: 0.8882\")\n",
        "\n",
        "print(\"\\n方法 2 - 混合 chunks (失敗):\")\n",
        "print(\"  Recall@1: 46.34%  |  Recall@5: 60.98%  |  MRR: 0.5305\")\n",
        "\n",
        "print(f\"\\n方法 3 - 智能混合檢索 (當前):\")\n",
        "print(f\"  Recall@1: {metrics['Recall@1']:.2%}  |  Recall@5: {metrics['Recall@5']:.2%}  |  MRR: {metrics['MRR']:.4f}\")\n",
        "\n",
        "print(\"\\n✨ 推薦使用：智能混合檢索\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_U8QgoHfOO2",
        "outputId": "967e56b3-b904-4526-f4f9-29386acdf0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 載入 chunks...\n",
            "\n",
            "✓ 純文字 chunks: 1778\n",
            "✓ 混合 chunks: 2608\n",
            "\n",
            "🔨 建立雙重 BM25 索引...\n",
            "\n",
            "✓ 純文字 BM25 索引完成\n",
            "✓ 表格 BM25 索引完成 (134 個表格)\n",
            "\n",
            "📊 評估混合檢索策略...\n",
            "\n",
            "  ✓ 完成 41 問題                    \n",
            "\n",
            "======================================================================\n",
            "📈 混合檢索效果（智能路由）\n",
            "======================================================================\n",
            "評估問題數: 41\n",
            "表格查詢數: 27 (65.9%)\n",
            "\n",
            "  Recall@ 1  : 85.37%  ██████████████████████████████████████████\n",
            "  Recall@ 3  : 90.24%  █████████████████████████████████████████████\n",
            "  Recall@ 5  : 90.24%  █████████████████████████████████████████████\n",
            "  Recall@10  : 90.24%  █████████████████████████████████████████████\n",
            "\n",
            "  MRR        : 0.8780\n",
            "======================================================================\n",
            "\n",
            "🔍 測試關鍵案例...\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📋 q054: What is the model size in gigabytes (GB) for the LLaMA-33B model?...\n",
            "✓ 期望: chen2024\n",
            "🔍 查詢類型: 表格查詢 📊\n",
            "\n",
            "檢索結果:\n",
            "  ✅ #1 | 📄 text   | text_index   |  27.36 | chen2024\n",
            "       For larger payload sizes, the primary factor influencing networking time is the utilization of network bandwidth. In thi...\n",
            "     #2 | 📄 text   | text_index   |  27.19 | erben2023\n",
            "     #3 | 📄 text   | text_index   |  26.59 | rubei2025\n",
            "  ✅ #4 | 📄 text   | text_index   |  26.54 | chen2024\n",
            "     #5 | 📄 text   | text_index   |  26.17 | kim2025\n",
            "\n",
            "  ✅ 成功！\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📋 q297: When comparing small and large versions of Qwen models in zero-shot classificati...\n",
            "✓ 期望: zschache2025\n",
            "🔍 查詢類型: 表格查詢 📊\n",
            "\n",
            "檢索結果:\n",
            "     #1 | 📄 text   | text_index   |  33.22 | luccioni2024\n",
            "     #2 | 📄 text   | text_index   |  32.50 | luccioni2024\n",
            "     #3 | 📄 text   | text_index   |  32.50 | luccioni2024\n",
            "     #4 | 📄 text   | text_index   |  32.29 | luccioni2024\n",
            "     #5 | 📄 text   | text_index   |  30.30 | luccioni2024\n",
            "\n",
            "  ❌ 失敗\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📋 q009: What were the net CO2e emissions from training the GShard-600B model?...\n",
            "✓ 期望: patterson2021\n",
            "🔍 查詢類型: 表格查詢 📊\n",
            "\n",
            "檢索結果:\n",
            "  ✅ #1 | 📄 text   | text_index   |  29.96 | patterson2021\n",
            "       already been used to translate billions of queries annually for each mid-to-low resource language 25 with 2B \n",
            "speakers g...\n",
            "  ✅ #2 | 📄 text   | text_index   |  27.88 | patterson2021\n",
            "       wide range of parallel computation patterns with minimal changes to the existing model code [Lep 20]. It \n",
            "enabled scalin...\n",
            "     #3 | 📄 text   | text_index   |  23.74 | wu2021a\n",
            "     #4 | 📄 text   | text_index   |  21.63 | li2025a\n",
            "     #5 | 📄 text   | text_index   |  21.33 | wu2021a\n",
            "\n",
            "  ✅ 成功！\n",
            "\n",
            "======================================================================\n",
            "📊 三種方法對比\n",
            "======================================================================\n",
            "\n",
            "方法 1 - 純文字 chunks:\n",
            "  Recall@1: 85.37%  |  Recall@5: 92.68%  |  MRR: 0.8882\n",
            "\n",
            "方法 2 - 混合 chunks (失敗):\n",
            "  Recall@1: 46.34%  |  Recall@5: 60.98%  |  MRR: 0.5305\n",
            "\n",
            "方法 3 - 智能混合檢索 (當前):\n",
            "  Recall@1: 85.37%  |  Recall@5: 90.24%  |  MRR: 0.8780\n",
            "\n",
            "✨ 推薦使用：智能混合檢索\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Run1 — Sparse Retrieval (BM25)**"
      ],
      "metadata": {
        "id": "OOqjNw9e5aV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_system(\n",
        "    qa_df: pd.DataFrame,\n",
        "    search_function,\n",
        "    system_name: str,\n",
        "    **search_kwargs\n",
        ") -> Dict:\n",
        "    print(f\"評估 {system_name}...\")\n",
        "\n",
        "    recalls = {k: [] for k in [1, 3, 5, 10]}\n",
        "    ndcgs = {k: [] for k in [1, 3, 5, 10]}\n",
        "    mrr_scores = []\n",
        "\n",
        "    for idx, row in qa_df.iterrows():\n",
        "        question = row['question']\n",
        "        ref_id_raw = row['ref_id']\n",
        "\n",
        "        if pd.isna(ref_id_raw) or str(ref_id_raw).strip() == '':\n",
        "            continue\n",
        "        try:\n",
        "            if isinstance(ref_id_raw, str):\n",
        "                if ref_id_raw.startswith('['):\n",
        "                    true_docs = ast.literal_eval(ref_id_raw)\n",
        "                    if not isinstance(true_docs, list):\n",
        "                        true_docs = [true_docs]\n",
        "                else:\n",
        "                    true_docs = [ref_id_raw.strip()]\n",
        "            else:\n",
        "                true_docs = [str(ref_id_raw).strip()]\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # 跳過trick questions\n",
        "        is_trick = any(doc in ['is_blank', ''] for doc in true_docs)\n",
        "        if is_trick:\n",
        "            continue\n",
        "\n",
        "        results = search_function(question, **search_kwargs)\n",
        "\n",
        "        retrieved_docs = []\n",
        "        seen = set()\n",
        "        for r in results:\n",
        "            doc_id = r['doc_id']\n",
        "            if doc_id not in seen:\n",
        "                seen.add(doc_id)\n",
        "                retrieved_docs.append(doc_id)\n",
        "\n",
        "        # 計算Recall@k\n",
        "        for k in [1, 3, 5, 10]:\n",
        "            top_k = retrieved_docs[:k]\n",
        "            hit = any(doc in top_k for doc in true_docs)\n",
        "            recalls[k].append(1 if hit else 0)\n",
        "\n",
        "        # 計算MRR\n",
        "        reciprocal_rank = 0.0\n",
        "        for rank, doc in enumerate(retrieved_docs, 1):\n",
        "            if doc in true_docs:\n",
        "                reciprocal_rank = 1.0 / rank\n",
        "                break\n",
        "        mrr_scores.append(reciprocal_rank)\n",
        "\n",
        "        # 計算nDCG@k\n",
        "        for k in [1, 3, 5, 10]:\n",
        "            ndcg = calculate_ndcg(retrieved_docs, true_docs, k)\n",
        "            ndcgs[k].append(ndcg)\n",
        "\n",
        "    metrics = {}\n",
        "    for k in [1, 3, 5, 10]:\n",
        "        metrics[f'Recall@{k}'] = np.mean(recalls[k]) if recalls[k] else 0.0\n",
        "        metrics[f'nDCG@{k}'] = np.mean(ndcgs[k]) if ndcgs[k] else 0.0\n",
        "    metrics['MRR'] = np.mean(mrr_scores) if mrr_scores else 0.0\n",
        "    metrics['num_queries'] = len(recalls[1])\n",
        "\n",
        "    print(f\"  ✓ 完成 ({metrics['num_queries']} queries)\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "v-vXpgGww4LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from rank_bm25 import BM25Okapi\n",
        "import ast\n",
        "# 注意：此模組依賴您在其他 Cell 中載入的 train_qa 和 chunks 變數（chunks = hybrid_chunks）\n",
        "\n",
        "# ============================================\n",
        "# 1. BM25 核心函數定義\n",
        "# ============================================\n",
        "\n",
        "def enhanced_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"增強分詞器：保留數字、年份、技術術語、bigrams\"\"\"\n",
        "    # 這是您決定保留的 Tokenizer 邏輯\n",
        "    text = text.lower()\n",
        "    tokens = []\n",
        "\n",
        "    numbers = re.findall(r'\\d+\\.?\\d*%?|\\d+[kmbt]?', text)\n",
        "    tokens.extend(numbers)\n",
        "\n",
        "    years = re.findall(r'\\b20\\d{2}\\b', text)\n",
        "    tokens.extend(years)\n",
        "\n",
        "    tech_terms = re.findall(\n",
        "        r'\\b(?:gpu|tpu|cpu|kwh|mwh|twh|co2|tco2e|gflops|teraflops|'\n",
        "        r'bert|gpt|llama|transformer|bm25|household|consumption)\\b',\n",
        "        text\n",
        "    )\n",
        "    tokens.extend(tech_terms)\n",
        "\n",
        "    words = re.findall(r'[a-z]+', text)\n",
        "    tokens.extend(words)\n",
        "\n",
        "    bigrams = [f\"{words[i]}_{words[i+1]}\" for i in range(len(words)-1)]\n",
        "    tokens.extend(bigrams[:5])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def build_bm25_enhanced(chunks: List[Dict]):\n",
        "    \"\"\"建立 BM25 索引物件 - ⭐ 已切換到 Hybrid Chunks\"\"\"\n",
        "    print(\"建立 BM25 Enhanced 索引...\")\n",
        "\n",
        "    # ⭐ 關鍵修改：將 corpus 來源切換到 'chunks' (hybrid data)\n",
        "    corpus = [chunk['content'] for chunk in chunks]\n",
        "    tokenized_corpus = [enhanced_tokenize(text) for text in corpus]\n",
        "\n",
        "    bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "    print(f\"✓ 完成 ({len(corpus)} chunks)\")\n",
        "    return bm25\n",
        "\n",
        "def search_bm25_enhanced(query: str, bm25: BM25Okapi, chunks: List[Dict], top_k: int = 10) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    執行 BM25 檢索 (被 evaluate_system 呼叫) - ⭐ 已切換到 Hybrid Chunks\n",
        "\n",
        "    注意：此處依賴的 chunks 參數已在 evaluate_system 中傳入\n",
        "    \"\"\"\n",
        "    tokenized_query = enhanced_tokenize(query)\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        # ⭐ 關鍵修改：從傳入的 chunks 參數中獲取內容\n",
        "        results.append({\n",
        "            'rank': rank,\n",
        "            'score': float(scores[idx]),\n",
        "            'doc_id': chunks[idx]['doc_id'],\n",
        "            'content': chunks[idx]['content'],\n",
        "            'source': 'bm25_enhanced'\n",
        "        })\n",
        "    return results\n",
        "def calculate_ndcg(retrieved_docs: List[str], true_docs: List[str], k: int) -> float:\n",
        "    \"\"\"計算nDCG@k\"\"\"\n",
        "    # DCG (Discounted Cumulative Gain)\n",
        "    dcg = 0.0\n",
        "    for i, doc in enumerate(retrieved_docs[:k], 1):\n",
        "        relevance = 1 if doc in true_docs else 0\n",
        "        dcg += relevance / np.log2(i + 1)\n",
        "\n",
        "    # IDCG (Ideal DCG)\n",
        "    ideal_docs = true_docs + ['dummy'] * k\n",
        "    idcg = 0.0\n",
        "    for i in range(1, min(len(true_docs), k) + 1):\n",
        "        idcg += 1.0 / np.log2(i + 1)\n",
        "\n",
        "    # nDCG\n",
        "    if idcg == 0:\n",
        "        return 0.0\n",
        "    return dcg / idcg\n",
        "\n",
        "# ============================================\n",
        "# 2. 建立索引 (讓 bm25_enhanced 變數生效)\n",
        "# ============================================\n",
        "print(\"=\"*70)\n",
        "print(\"Run1: BM25 Enhanced 核心模組\")\n",
        "print(\"=\"*70)\n",
        "bm25_enhanced = build_bm25_enhanced(chunks) # ⭐ 傳入 chunks\n",
        "print()\n",
        "\n",
        "# ============================================\n",
        "# 3. 評估與結果展示 (修正後的執行順序)\n",
        "# ============================================\n",
        "\n",
        "# ⭐ 關鍵：evaluate_system 必須傳入 chunks 變數\n",
        "metrics_run1 = evaluate_system(\n",
        "    train_qa,\n",
        "    search_bm25_enhanced,\n",
        "    \"Run1: BM25 Enhanced (Hybrid Data)\",\n",
        "    bm25=bm25_enhanced,\n",
        "    chunks=chunks, # ⭐ 傳入 chunks 列表\n",
        "    top_k=10\n",
        ")\n",
        "\n",
        "print()\n",
        "print(\"=\"*70)\n",
        "print(\"📊 評估結果 (Run 1: BM25 Enhanced)\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "# ... (後續的 DataFrame 顯示邏輯) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CTGeHEqx38X",
        "outputId": "6291476b-8c37-4694-a34e-de028fcaa402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Run1: BM25 Enhanced 核心模組\n",
            "======================================================================\n",
            "建立 BM25 Enhanced 索引...\n",
            "✓ 完成 (63 chunks)\n",
            "\n",
            "評估 Run1: BM25 Enhanced (Hybrid Data)...\n",
            "  ✓ 完成 (39 queries)\n",
            "\n",
            "======================================================================\n",
            "📊 評估結果 (Run 1: BM25 Enhanced)\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# I. 定義雙重索引和搜索函數 (假設 text_chunks & chunks 均已載入)\n",
        "# -----------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List, Dict\n",
        "\n",
        "# 注意：這些函數依賴於您前面定義的 enhanced_tokenize\n",
        "# 假設 enhanced_tokenize, calculate_ndcg, evaluate_system, train_qa 已經可用\n",
        "\n",
        "def build_bm25_index_for_comparison(data_list: List[Dict]):\n",
        "    \"\"\"根據傳入的列表建立 BM25 索引\"\"\"\n",
        "    corpus = [chunk['content'] for chunk in data_list]\n",
        "    tokenized_corpus = [enhanced_tokenize(text) for text in corpus]\n",
        "    return BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "\n",
        "def search_bm25_generic(query: str, bm25_index: BM25Okapi, data_list: List[Dict], top_k: int = 10) -> List[Dict]:\n",
        "    \"\"\"通用 BM25 檢索函數 (傳入 index 和 data source)\"\"\"\n",
        "    tokenized_query = enhanced_tokenize(query)\n",
        "    scores = bm25_index.get_scores(tokenized_query)\n",
        "\n",
        "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        chunk = data_list[idx]\n",
        "        results.append({\n",
        "            'rank': rank,\n",
        "            'score': float(scores[idx]),\n",
        "            'doc_id': chunk['doc_id'],\n",
        "            'content': chunk['content'],\n",
        "            'source': 'bm25_comp'\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# II. 建立兩個獨立的索引\n",
        "# -----------------------------------------------------------\n",
        "print(\"=\"*70)\n",
        "print(\"🔨 建立雙重索引...\")\n",
        "print(\"=\"*70)\n",
        "chunks = hybrid_chunks\n",
        "# 1. Hybrid Data Index\n",
        "bm25_hybrid = build_bm25_index_for_comparison(chunks) # 假設 chunks = hybrid_chunks\n",
        "print(f\"✓ Hybrid Index 完成 ({len(chunks)} chunks)\")\n",
        "\n",
        "# 2. Text-Only Index\n",
        "bm25_text = build_bm25_index_for_comparison(text_chunks) # 假設 text_chunks = 舊純文本\n",
        "print(f\"✓ Text-Only Index 完成 ({len(text_chunks)} chunks)\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# III. 執行雙重評估\n",
        "# -----------------------------------------------------------\n",
        "print(\"\\n🔄 執行雙重評估 (Run Hybrid vs Run Text-Only)\")\n",
        "\n",
        "# 評估 Run A (Hybrid Data - 結構化新數據源)\n",
        "metrics_hybrid = evaluate_system(\n",
        "    train_qa,\n",
        "    search_bm25_generic,\n",
        "    \"Run A: Hybrid Data (NEW)\",\n",
        "    bm25_index=bm25_hybrid,\n",
        "    data_list=chunks,\n",
        "    top_k=10\n",
        ")\n",
        "\n",
        "print()\n",
        "\n",
        "# 評估 Run B (Text-Only - 扁平化舊數據源)\n",
        "metrics_text = evaluate_system(\n",
        "    train_qa,\n",
        "    search_bm25_generic,\n",
        "    \"Run B: Text-Only (OLD)\",\n",
        "    bm25_index=bm25_text,\n",
        "    data_list=text_chunks,\n",
        "    top_k=10\n",
        ")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# IV. 顯示最終對比表格\n",
        "# -----------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 BM25 資料源對比結果\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_data = []\n",
        "for metric in ['Recall@1', 'Recall@3', 'MRR', 'nDCG@5']:\n",
        "    r_hybrid = metrics_hybrid[metric]\n",
        "    r_text = metrics_text[metric]\n",
        "\n",
        "    best_val = max(r_hybrid, r_text)\n",
        "\n",
        "    comparison_data.append({\n",
        "        'Metric': metric,\n",
        "        'Hybrid Data (A)': f\"{r_hybrid:.4f}\" + (\"*\" if r_hybrid == best_val else \"\"),\n",
        "        'Text Only (B)': f\"{r_text:.4f}\" + (\"*\" if r_text == best_val else \"\"),\n",
        "        'Delta (A-B)': f\"{r_hybrid - r_text:+.4f}\"\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"─\"*70)\n",
        "if metrics_hybrid['Recall@1'] >= metrics_text['Recall@1']:\n",
        "    print(\"✅ 結論：Hybrid Data 的 Recall 佔優或持平。維持使用 Hybrid Data。\")\n",
        "else:\n",
        "    print(\"⚠️ 結論：Text-Only Data  Recall 仍較高。需要分析 Hybrid Chunks 移除的資料。\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiJd6wtlCPP9",
        "outputId": "bf027092-7a05-4257-a01a-e3471b9630c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "🔨 建立雙重索引...\n",
            "======================================================================\n",
            "✓ Hybrid Index 完成 (2608 chunks)\n",
            "✓ Text-Only Index 完成 (1778 chunks)\n",
            "\n",
            "🔄 執行雙重評估 (Run Hybrid vs Run Text-Only)\n",
            "評估 Run A: Hybrid Data (NEW)...\n",
            "  ✓ 完成 (39 queries)\n",
            "\n",
            "評估 Run B: Text-Only (OLD)...\n",
            "  ✓ 完成 (39 queries)\n",
            "\n",
            "======================================================================\n",
            "📊 BM25 資料源對比結果\n",
            "======================================================================\n",
            "  Metric Hybrid Data (A) Text Only (B) Delta (A-B)\n",
            "Recall@1          0.7949       0.8718*     -0.0769\n",
            "Recall@3          0.8974       0.9231*     -0.0256\n",
            "     MRR          0.8526       0.8974*     -0.0449\n",
            "  nDCG@5          0.8564       0.8859*     -0.0295\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "⚠️ 結論：Text-Only Data  Recall 仍較高。需要分析 Hybrid Chunks 移除的資料。\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_q = pd.read_csv(INPUT_DIR / \"test_Q.csv\", encoding=\"latin1\")\n",
        "print(\"欄位名稱:\", test_q.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZOO93niY3ZU",
        "outputId": "68f7137b-294a-4caf-af65-f5e28cd37e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "欄位名稱: ['ï»¿id', 'question', 'answer', 'answer_value', 'answer_unit', 'ref_id', 'ref_url', 'supporting_materials', 'explanation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AG"
      ],
      "metadata": {
        "id": "R60v4YwlCulc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# ============================================================================\n",
        "# 0. API 設定\n",
        "# ============================================================================\n",
        "\n",
        "GEMINI_API_KEY = \"# ⚠️ 請替換成你的 API Key\"  # ⚠️ 請替換成你的 API Key\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"❌ GEMINI_API_KEY 環境變數沒設到\")\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    \"gemini-2.5-flash\": {\n",
        "        \"input_price\": 0.075,\n",
        "        \"output_price\": 0.30,\n",
        "        \"rpm_limit\": 1000,\n",
        "        \"description\": \"Gemini 2.5 Flash - 高速 (RPM 1000)\"\n",
        "    },\n",
        "    \"gemini-2.5-pro\": {\n",
        "        \"input_price\": 1.25,\n",
        "        \"output_price\": 5.0,\n",
        "        \"rpm_limit\": 150,\n",
        "        \"description\": \"Gemini 2.5 Pro - 高品質 (RPM 150)\"\n",
        "    },\n",
        "    \"gemini-3-pro-preview\": {\n",
        "        \"input_price\": 2.0,\n",
        "        \"output_price\": 12.0,\n",
        "        \"rpm_limit\": 50,\n",
        "        \"description\": \"Gemini 3.0 Pro Preview - 次世代 (RPM 50)\"\n",
        "    },\n",
        "    \"gemini-2.0-flash\": {\n",
        "        \"input_price\": 0.10,\n",
        "        \"output_price\": 0.40,\n",
        "        \"rpm_limit\": 2000,\n",
        "        \"description\": \"Gemini 2.0 Flash (RPM 2000)\"\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. 問題類型檢測器\n",
        "# ============================================================================\n",
        "\n",
        "def detect_question_type(question: str) -> Tuple[str, bool]:\n",
        "    \"\"\"\n",
        "    檢測問題類型，返回 (問題類型, 是否需要激進策略)\n",
        "\n",
        "    Returns:\n",
        "        (question_type, needs_aggressive_mode)\n",
        "    \"\"\"\n",
        "    q_lower = question.lower()\n",
        "\n",
        "    # 類型 1: 表格數值提取 (最容易失敗)\n",
        "    table_extraction_patterns = [\n",
        "        r'execution time',\n",
        "        r'total time',\n",
        "        r'throughput.*queries',\n",
        "        r'queries.*per.*sec',\n",
        "        r'batch size',\n",
        "        r'maximum batch',\n",
        "        r'gpu hours',\n",
        "        r'h100.*hours',\n",
        "        r'a100.*hours',\n",
        "        r'total.*parameters',\n",
        "        r'number of parameters',\n",
        "        r'number of experts',\n",
        "        r'how many experts',\n",
        "        r'energy consumption.*wh',\n",
        "        r'power usage.*kwh',\n",
        "        r'cost.*\\$|cost.*usd',\n",
        "        r'price per hour',\n",
        "        r'energy.*mwh|energy.*kwh',\n",
        "        r'what is the .* for .* model',\n",
        "    ]\n",
        "    for pattern in table_extraction_patterns:\n",
        "        if re.search(pattern, q_lower):\n",
        "            return (\"table_extraction\", True)\n",
        "\n",
        "    # 類型 2: 特定模型/論文的數據\n",
        "    specific_model_patterns = [\n",
        "        (r'jetmoe', \"model_specific\"),\n",
        "        (r'blackmamba', \"model_specific\"),\n",
        "        (r'mixtral', \"model_specific\"),\n",
        "        (r'flm-101b', \"model_specific\"),\n",
        "        (r'bloom\\b', \"model_specific\"),\n",
        "        (r'olmo', \"model_specific\"),\n",
        "        (r'llama.?\\d', \"model_specific\"),\n",
        "    ]\n",
        "    for pattern, qtype in specific_model_patterns:\n",
        "        if re.search(pattern, q_lower):\n",
        "            return (qtype, True)\n",
        "\n",
        "    # 類型 3: 分數/排名問題\n",
        "    score_patterns = [\n",
        "        r'what score',\n",
        "        r'average score',\n",
        "        r'final.* score',\n",
        "        r'benchmark.*score',\n",
        "        r'accuracy',\n",
        "        r'performance score',\n",
        "        r'leaderboard',\n",
        "        r'gsm8k',\n",
        "        r'mt-bench',\n",
        "        r'bleu',\n",
        "    ]\n",
        "    for pattern in score_patterns:\n",
        "        if re.search(pattern, q_lower):\n",
        "            return (\"score_extraction\", True)\n",
        "\n",
        "    # 類型 4: 術語/名稱問題\n",
        "    term_patterns = [\n",
        "        r'what is the name of',\n",
        "        r'what is the term',\n",
        "        r'which (?:software|framework|tool|package|metric)',\n",
        "        r'what (?:software|framework|tool|package|metric)',\n",
        "        r'what.*called',\n",
        "    ]\n",
        "    for pattern in term_patterns:\n",
        "        if re.search(pattern, q_lower):\n",
        "            return (\"term_extraction\", True)\n",
        "\n",
        "    # 類型 5: 數量/計數問題 (通常有明確答案)\n",
        "    count_patterns = [\n",
        "        r'how many',\n",
        "        r'how much',\n",
        "        r'what percentage',\n",
        "        r'what fraction',\n",
        "        r'what proportion',\n",
        "        r'by what factor',\n",
        "        r'what was the .* increase',\n",
        "        r'what was the .* decrease',\n",
        "    ]\n",
        "    for pattern in count_patterns:\n",
        "        if re.search(pattern, q_lower):\n",
        "            return (\"numeric_extraction\", True)\n",
        "\n",
        "    return (\"general\", False)\n",
        "\n",
        "\n",
        "def detect_multi_doc_question(question: str) -> Tuple[bool, str]:\n",
        "    \"\"\"保守地偵測是否需要多篇論文\"\"\"\n",
        "    question_lower = question.lower()\n",
        "\n",
        "    cross_calc_patterns = [\n",
        "        r'equivalent to .+ (household|american|person|life)',\n",
        "        r'how many .+ equivalent to',\n",
        "        r'compare.*to',\n",
        "    ]\n",
        "    for pattern in cross_calc_patterns:\n",
        "        if re.search(pattern, question_lower):\n",
        "            return True, \"cross_doc_calculation\"\n",
        "\n",
        "    if 'and how does this compare' in question_lower:\n",
        "        return True, \"explicit_comparison\"\n",
        "\n",
        "    return False, \"single_doc\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. 增強版表格解析 Prompt\n",
        "# ============================================================================\n",
        "\n",
        "def get_enhanced_table_prompt() -> str:\n",
        "    \"\"\"返回增強版的表格解析指引\"\"\"\n",
        "    return \"\"\"\n",
        "📊 ENHANCED TABLE PARSING RULES (CRITICAL):\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "1. TABLE STRUCTURE:\n",
        "   - Tables use \"|\" to separate columns\n",
        "   - First row = HEADER, subsequent rows = DATA\n",
        "   - Example:\n",
        "     | Model | Batch Size | Time (s) | Cost ($) |\n",
        "     |---------|---------------|------------|-------------|\n",
        "     | Dense |     30  |   45.2 |   12.5  |\n",
        "     | Sparse |     84  |   32.1 |    8.7 |\n",
        "\n",
        "2. HOW TO READ:\n",
        "   - STEP 1: Find ROW matching question criteria (model, config)\n",
        "   - STEP 2: Find COLUMN containing the answer type\n",
        "   - STEP 3: Extract VALUE at intersection\n",
        "   - STEP 4: Include UNIT from column header\n",
        "\n",
        "3. CRITICAL MATCHING:\n",
        "   - \"dense\" vs \"sparse\" → DIFFERENT rows!\n",
        "   - \"BlackMamba\" vs \"Mixtral\" → DIFFERENT models!\n",
        "   - \"A40\" vs \"A100\" vs \"H100\" → DIFFERENT GPUs!\n",
        "   - \"batch size = 1\" vs \"batch size = 30\" → DIFFERENT rows!\n",
        "\n",
        "4. IF YOU SEE A TABLE:\n",
        "   - Check ALL rows before saying \"Unable to answer\"\n",
        "   - The answer IS there if the model/config is mentioned\n",
        "   - Extract EXACT number with full precision\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. BM25 檢索模組\n",
        "# ============================================================================\n",
        "\n",
        "def enhanced_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"增強分詞器：保留數字、年份、技術術語\"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = []\n",
        "\n",
        "    # 數字 (含小數、百分比)\n",
        "    tokens.extend(re.findall(r'\\d+\\.?\\d*%?', text))\n",
        "\n",
        "    # 年份\n",
        "    tokens.extend(re.findall(r'\\b20\\d{2}\\b', text))\n",
        "\n",
        "    # 技術術語\n",
        "    tokens.extend(re.findall(\n",
        "        r'\\b(?:gpu|tpu|cpu|kwh|mwh|twh|co2|tco2e|gflops|teraflops|'\n",
        "        r'bert|gpt|llama|transformer|household|consumption|'\n",
        "        r'jetmoe|blackmamba|mixtral|bloom|inference|training)\\b',\n",
        "        text\n",
        "    ))\n",
        "\n",
        "    # 一般單詞\n",
        "    words = re.findall(r'[a-z]+', text)\n",
        "    tokens.extend(words)\n",
        "\n",
        "    # Bigrams (前5個)\n",
        "    tokens.extend([f\"{words[i]}_{words[i+1]}\" for i in range(min(5, len(words)-1))])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def build_bm25_index(chunks: List[Dict]) -> BM25Okapi:\n",
        "    \"\"\"建立 BM25 索引\"\"\"\n",
        "    print(\"\\n🔨 建立 BM25 索引...\")\n",
        "    corpus = [chunk['content'] for chunk in chunks]\n",
        "    tokenized_corpus = [enhanced_tokenize(text) for text in corpus]\n",
        "    bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "    print(f\"✓ BM25 索引完成 ({len(corpus)} chunks)\")\n",
        "    return bm25\n",
        "\n",
        "\n",
        "def search_bm25(query: str, bm25: BM25Okapi, chunks: List[Dict], top_k: int = 10) -> List[Dict]:\n",
        "    \"\"\"BM25 檢索\"\"\"\n",
        "    tokenized_query = enhanced_tokenize(query)\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        chunk = chunks[idx]\n",
        "        results.append({\n",
        "            'rank': rank,\n",
        "            'score': float(scores[idx]),\n",
        "            'doc_id': chunk['doc_id'],\n",
        "            'content': chunk['content'],\n",
        "            'type': chunk.get('type', 'text')\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. 增強版 Answer Generator (Gemini API)\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedTwoStageGPTGenerator:\n",
        "    \"\"\"\n",
        "    增強版兩階段生成器 - Gemini API\n",
        "\n",
        "    改進:\n",
        "    1. 問題類型檢測 → 自動選擇激進/保守策略\n",
        "    2. 增強表格解析 prompt\n",
        "    3. 對特定問題類型降低 \"Unable to answer\" 門檻\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, model: str = \"gemini-2.5-pro\", client=None, rpm_limit: int = None):\n",
        "        self.model = model\n",
        "        self.client = client\n",
        "        self.call_count = 0\n",
        "        self.total_cost = 0.0\n",
        "        self.stage1_success = 0\n",
        "        self.stage2_retry = 0\n",
        "        self.multi_doc_count = 0\n",
        "        self.aggressive_count = 0\n",
        "        self.no_text_count = 0\n",
        "\n",
        "        # 此函數是為了讓Gemini能快速切換模型才建立\n",
        "        if model in MODEL_CONFIGS:\n",
        "            self.model_config = MODEL_CONFIGS[model]\n",
        "            self.rpm_limit = rpm_limit or self.model_config['rpm_limit']\n",
        "            print(f\"📌 使用模型: {model}\")\n",
        "            print(f\"   {self.model_config['description']}\")\n",
        "            print(f\"   定價: ${self.model_config['input_price']}/${self.model_config['output_price']} per 1M tokens\")\n",
        "            print(f\"   Rate Limit: {self.rpm_limit}/min\")\n",
        "        else:\n",
        "            print(f\"⚠️ 未知模型 {model}，使用默認配置\")\n",
        "            self.model_config = MODEL_CONFIGS[\"gemini-2.5-flash\"]\n",
        "            self.rpm_limit = rpm_limit or 150\n",
        "\n",
        "        self.call_timestamps = []\n",
        "\n",
        "    #為了應對Gemini API 每分鐘請求上限\n",
        "    def _wait_for_rate_limit(self):\n",
        "        \"\"\"確保不超過rate limit\"\"\"\n",
        "        now = datetime.now()\n",
        "\n",
        "        self.call_timestamps = [\n",
        "            ts for ts in self.call_timestamps\n",
        "            if now - ts < timedelta(minutes=1)\n",
        "        ]\n",
        "\n",
        "        if len(self.call_timestamps) >= self.rpm_limit:\n",
        "            oldest_call = min(self.call_timestamps)\n",
        "            wait_time = 60 - (now - oldest_call).total_seconds()\n",
        "            if wait_time > 0:\n",
        "                print(f\"  ⏳ Rate limit, waiting {wait_time:.1f}s...\")\n",
        "                time.sleep(wait_time + 1)\n",
        "\n",
        "        self.call_timestamps.append(datetime.now())\n",
        "\n",
        "    def generate_answer(\n",
        "        self,\n",
        "        question: str,\n",
        "        retrieval_results: List[Dict],\n",
        "        expected_unit: str = \"is_blank\"\n",
        "    ) -> Dict:\n",
        "        \"\"\"兩階段生成主函數\"\"\"\n",
        "\n",
        "        # 問題類型檢測\n",
        "        question_type, needs_aggressive = detect_question_type(question)\n",
        "        if needs_aggressive:\n",
        "            self.aggressive_count += 1\n",
        "\n",
        "        # 多文檔檢測\n",
        "        is_multi_doc, reason = detect_multi_doc_question(question)\n",
        "        if is_multi_doc:\n",
        "            self.multi_doc_count += 1\n",
        "\n",
        "        # ============================================\n",
        "        # Stage 1: Top-3 + 根據問題類型選擇策略\n",
        "        # ============================================\n",
        "        top3_results = retrieval_results[:3]\n",
        "        context_top3 = self._build_context(top3_results)\n",
        "\n",
        "        stage1_response = self._call_gpt(\n",
        "            question=question,\n",
        "            context=context_top3,\n",
        "            retrieval_results=top3_results,\n",
        "            expected_unit=expected_unit,\n",
        "            mode=\"aggressive\" if needs_aggressive else \"strict\",\n",
        "            question_type=question_type,\n",
        "            is_multi_doc=is_multi_doc\n",
        "        )\n",
        "\n",
        "        is_unable = \"Unable to answer\" in stage1_response.get('answer', '')\n",
        "\n",
        "        if not is_unable:\n",
        "            self.stage1_success += 1\n",
        "            return stage1_response\n",
        "\n",
        "        # ============================================\n",
        "        # Stage 2: 擴大範圍 + 更激進的策略\n",
        "        # ============================================\n",
        "        self.stage2_retry += 1\n",
        "\n",
        "        # 對特定問題類型使用更多 context\n",
        "        top_k = 10 if needs_aggressive else 7\n",
        "        top_results = retrieval_results[:top_k]\n",
        "        context_expanded = self._build_context(top_results)\n",
        "\n",
        "        stage2_response = self._call_gpt(\n",
        "            question=question,\n",
        "            context=context_expanded,\n",
        "            retrieval_results=top_results,\n",
        "            expected_unit=expected_unit,\n",
        "            mode=\"aggressive\",  # Stage 2 永遠使用激進模式\n",
        "            question_type=question_type,\n",
        "            is_multi_doc=is_multi_doc\n",
        "        )\n",
        "\n",
        "        return stage2_response\n",
        "\n",
        "    def _build_context(self, results: List[Dict]) -> str:\n",
        "        \"\"\"構建上下文 - 增強表格標記\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        for i, r in enumerate(results, 1):\n",
        "            content = r['content']\n",
        "            chunk_type = r.get('type', 'text')\n",
        "            doc_id = r['doc_id']\n",
        "\n",
        "            if chunk_type == 'table':\n",
        "                prefix = (\n",
        "                    f\"[DOCUMENT #{i}: {doc_id}]\\n\"\n",
        "                    f\"⚠️ DATA TABLE - READ CAREFULLY:\\n\"\n",
        "                    f\"{'='*60}\\n\"\n",
        "                )\n",
        "            else:\n",
        "                prefix = f\"[DOCUMENT #{i}: {doc_id}]\\n\"\n",
        "\n",
        "            context_parts.append(prefix + content)\n",
        "\n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    # 因為Gemini 每版本的JSON輸出模式不同，因此設置此函數抓取正確回覆來源\n",
        "    def _extract_text_from_response(self, response) -> Optional[str]:\n",
        "        \"\"\"從 Gemini 回應中提取文字，兼容 2.5 / 3.0\"\"\"\n",
        "        try:\n",
        "            # 路徑 1：最簡單，先試 response.text\n",
        "            try:\n",
        "                text = response.text\n",
        "            except Exception:\n",
        "                text = None\n",
        "\n",
        "            if isinstance(text, str) and text.strip():\n",
        "                return text.strip()\n",
        "\n",
        "            # 路徑 2：Gemini 3 推薦的 response.parts\n",
        "            if hasattr(response, \"parts\") and response.parts:\n",
        "                text_parts = []\n",
        "                for part in response.parts:\n",
        "                    part_text = getattr(part, \"text\", None)\n",
        "                    if isinstance(part_text, str) and part_text.strip():\n",
        "                        text_parts.append(part_text.strip())\n",
        "                if text_parts:\n",
        "                    return \"\\n\".join(text_parts)\n",
        "\n",
        "            # 路徑 3：舊版格式 candidates[0].content.parts\n",
        "            if hasattr(response, \"candidates\") and response.candidates:\n",
        "                for cand in response.candidates:\n",
        "                    content = getattr(cand, \"content\", None)\n",
        "                    if content is None:\n",
        "                        continue\n",
        "\n",
        "                    parts = getattr(content, \"parts\", None)\n",
        "                    if not parts:\n",
        "                        continue\n",
        "\n",
        "                    text_parts = []\n",
        "                    for part in parts:\n",
        "                        part_text = getattr(part, \"text\", None)\n",
        "                        if isinstance(part_text, str) and part_text.strip():\n",
        "                            text_parts.append(part_text.strip())\n",
        "\n",
        "                    if text_parts:\n",
        "                        return \"\\n\".join(text_parts)\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ 提取文本異常: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _call_gpt(\n",
        "        self,\n",
        "        question: str,\n",
        "        context: str,\n",
        "        retrieval_results: List[Dict],\n",
        "        expected_unit: str,\n",
        "        mode: str,\n",
        "        question_type: str,\n",
        "        is_multi_doc: bool\n",
        "    ) -> Dict:\n",
        "        \"\"\"調用 Gemini API\"\"\"\n",
        "\n",
        "        if mode == \"aggressive\":\n",
        "            system_prompt = self._get_aggressive_system_prompt(question_type, is_multi_doc)\n",
        "        else:\n",
        "            system_prompt = self._get_strict_system_prompt(is_multi_doc)\n",
        "\n",
        "        user_prompt = self._build_user_prompt(\n",
        "            question, context, expected_unit, retrieval_results, question_type, is_multi_doc\n",
        "        )\n",
        "\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                self._wait_for_rate_limit()\n",
        "\n",
        "                full_prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "---\n",
        "\n",
        "{user_prompt}\"\"\"\n",
        "\n",
        "                # Safety Settings，此段落為新增，讓 Gemini 不要過濾問題\n",
        "                safety_settings = [\n",
        "                    types.SafetySetting(\n",
        "                        category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "                        threshold=\"BLOCK_NONE\"\n",
        "                    ),\n",
        "                    types.SafetySetting(\n",
        "                        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "                        threshold=\"BLOCK_NONE\"\n",
        "                    ),\n",
        "                    types.SafetySetting(\n",
        "                        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "                        threshold=\"BLOCK_NONE\"\n",
        "                    ),\n",
        "                    types.SafetySetting(\n",
        "                        category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "                        threshold=\"BLOCK_NONE\"\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "                # Generation Config\n",
        "                gen_config = types.GenerateContentConfig(\n",
        "                    temperature=0.1,\n",
        "                    max_output_tokens=8192,\n",
        "                    response_mime_type=\"application/json\",\n",
        "                    safety_settings=safety_settings\n",
        "                )\n",
        "\n",
        "                response = self.client.models.generate_content(\n",
        "                    model=self.model,\n",
        "                    contents=full_prompt,\n",
        "                    config=gen_config,\n",
        "                )\n",
        "\n",
        "                self.call_count += 1\n",
        "\n",
        "                content = self._extract_text_from_response(response)\n",
        "\n",
        "                # 處理無文本回應\n",
        "                if content is None or content.strip() == '':\n",
        "                    self.no_text_count += 1\n",
        "\n",
        "                    # 抓取被擋掉的原因\n",
        "                    reason = \"UNKNOWN\"\n",
        "                    try:\n",
        "                        if response.candidates:\n",
        "                            reason = response.candidates[0].finish_reason\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    print(f\"  ⚠️ 無文本 (原因: {reason}) (累計 {self.no_text_count})\")\n",
        "\n",
        "                    # 如果是版權問題，直接跳過\n",
        "                    if str(reason) == \"RECITATION\":\n",
        "                        print(\"  🛑 偵測到版權引用限制 (Recitation)，跳過重試\")\n",
        "                        return self._default_answer(expected_unit)\n",
        "\n",
        "                    if retry_count < max_retries - 1:\n",
        "                        print(f\"  🔄 重試...\")\n",
        "                        retry_count += 1\n",
        "                        time.sleep(3)\n",
        "                        continue\n",
        "                    else:\n",
        "                        return self._default_answer(expected_unit)\n",
        "\n",
        "                result = self._parse_response(content, expected_unit)\n",
        "\n",
        "                # 計算成本\n",
        "                try:\n",
        "                    input_tokens = response.usage_metadata.prompt_token_count\n",
        "                    output_tokens = response.usage_metadata.candidates_token_count\n",
        "                    input_price = self.model_config['input_price']\n",
        "                    output_price = self.model_config['output_price']\n",
        "                    cost = (input_tokens * input_price + output_tokens * output_price) / 1_000_000\n",
        "                    self.total_cost += cost\n",
        "                except:\n",
        "                    self.total_cost += 0.001\n",
        "\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                error_str = str(e)\n",
        "                if \"429\" in error_str or \"RESOURCE_EXHAUSTED\" in error_str:\n",
        "                    retry_count += 1\n",
        "                    if retry_count < max_retries:\n",
        "                        wait_time = min(10 * (2 ** retry_count), 60)\n",
        "                        print(f\"  ⏳ Rate limit (retry {retry_count}/{max_retries}), waiting {wait_time}s...\")\n",
        "                        time.sleep(wait_time)\n",
        "                        continue\n",
        "\n",
        "                print(f\"  ❌ API 錯誤: {e}\")\n",
        "                if retry_count < max_retries - 1:\n",
        "                    retry_count += 1\n",
        "                    continue\n",
        "                return self._default_answer(expected_unit)\n",
        "\n",
        "        return self._default_answer(expected_unit)\n",
        "\n",
        "#此段與原版相同\n",
        "    def _get_aggressive_system_prompt(self, question_type: str, is_multi_doc: bool) -> str:\n",
        "        \"\"\"激進模式 System Prompt\"\"\"\n",
        "\n",
        "        table_section = get_enhanced_table_prompt()\n",
        "\n",
        "        multi_doc_section = \"\"\"\n",
        "⭐ MULTI-DOCUMENT CITATION:\n",
        "- Combine data from multiple documents if needed\n",
        "- List all sources: [\"doc1\", \"doc2\"]\n",
        "\"\"\" if is_multi_doc else \"\"\n",
        "\n",
        "        # 問題類型特定提示\n",
        "        type_hints = {\n",
        "            \"table_extraction\": \"\"\"\n",
        "🎯 TABLE EXTRACTION MODE:\n",
        "- The answer IS in a table - search ALL rows carefully!\n",
        "- Match: model name + GPU type + batch size + dense/sparse\n",
        "- DO NOT say \"Unable to answer\" if you see relevant tables!\n",
        "\"\"\",\n",
        "            \"model_specific\": \"\"\"\n",
        "🎯 MODEL-SPECIFIC MODE:\n",
        "- Look for data specific to this MODEL\n",
        "- Common data: parameter counts, expert counts, scores, GPU hours, tokens\n",
        "- If model name appears in context, the answer should be there!\n",
        "\"\"\",\n",
        "            \"score_extraction\": \"\"\"\n",
        "🎯 SCORE EXTRACTION MODE:\n",
        "- Look for benchmark tables or score mentions\n",
        "- Common: GSM8k, MT-Bench, OpenLLM Leaderboard, accuracy, BLEU\n",
        "- Extract the EXACT number with full precision\n",
        "\"\"\",\n",
        "            \"term_extraction\": \"\"\"\n",
        "🎯 TERM/NAME EXTRACTION MODE:\n",
        "- Look for definitions, tool names, framework names, metrics\n",
        "- The answer is a proper noun or technical term\n",
        "- Search for quoted or emphasized text\n",
        "\"\"\",\n",
        "            \"numeric_extraction\": \"\"\"\n",
        "🎯 NUMERIC EXTRACTION MODE:\n",
        "- Look for specific numbers, percentages, counts\n",
        "- The answer exists in the documents\n",
        "- Extract the exact value mentioned\n",
        "\"\"\",\n",
        "        }\n",
        "\n",
        "        type_hint = type_hints.get(question_type, \"\")\n",
        "\n",
        "        return f\"\"\"⚠️ JSON OUTPUT RULES:\n",
        "- Output a single valid JSON object\n",
        "- ALL fields: answer, answer_value, answer_unit, supporting_text, explanation, source_doc_ids\n",
        "- Use \"is_blank\" for empty values (never null)\n",
        "- source_doc_ids = LIST: [\"doc_id\"]\n",
        "\n",
        "{type_hint}\n",
        "{table_section}\n",
        "{multi_doc_section}\n",
        "\n",
        "⚠️ AGGRESSIVE MODE - REDUCED \"UNABLE TO ANSWER\" THRESHOLD:\n",
        "- Only say \"Unable to answer\" if CERTAIN the info doesn't exist\n",
        "- If you see ANY relevant data, extract it!\n",
        "- Partial matches acceptable - use best available info\n",
        "- For numerical questions, extract the closest matching value\n",
        "- DO NOT refuse to answer if you see the model/topic mentioned!\n",
        "\n",
        "🔢 UNIT HANDLING:\n",
        "- MWh → kWh: ×1000\n",
        "- kWh → Wh: ×1000\n",
        "- million → raw: ×1,000,000\n",
        "- billion → raw: ×1,000,000,000\n",
        "- Always check if conversion is needed!\n",
        "\n",
        "🧠 EXPLANATION:\n",
        "- For table data: \"Found in row X, column Y of table\"\n",
        "- Show calculations if needed\n",
        "- Cite the specific text/number you found\n",
        "\n",
        "You are a data extraction expert. PREFER ANSWERING over refusing.\"\"\"\n",
        "\n",
        "    def _get_strict_system_prompt(self, is_multi_doc: bool) -> str:\n",
        "        \"\"\"嚴格模式 System Prompt\"\"\"\n",
        "\n",
        "        table_section = get_enhanced_table_prompt()\n",
        "\n",
        "        multi_doc_section = \"\"\"\n",
        "⭐ MULTI-DOCUMENT CITATION:\n",
        "- List sources as: [\"doc1\", \"doc2\"]\n",
        "\"\"\" if is_multi_doc else \"\"\n",
        "\n",
        "        return f\"\"\"⚠️ JSON OUTPUT RULES:\n",
        "- Output a single valid JSON object\n",
        "- ALL fields: answer, answer_value, answer_unit, supporting_text, explanation, source_doc_ids\n",
        "- Use \"is_blank\" for empty values\n",
        "- source_doc_ids = LIST\n",
        "\n",
        "⚠️ UNABLE TO ANSWER:\n",
        "- If cannot answer: answer = \"Unable to answer with confidence based on the provided documents.\"\n",
        "- answer_value = \"is_blank\", source_doc_ids = []\n",
        "- DO NOT guess or fabricate\n",
        "{multi_doc_section}\n",
        "{table_section}\n",
        "\n",
        "🔢 UNIT CONVERSION:\n",
        "- MWh → kWh: ×1000\n",
        "- million → raw: ×1,000,000\n",
        "\n",
        "✅ STRICT MODE: Answer only if evidence is explicit and clear.\n",
        "\n",
        "You are a scientific evidence extraction expert.\"\"\"\n",
        "\n",
        "    def _build_user_prompt(\n",
        "        self,\n",
        "        question: str,\n",
        "        context: str,\n",
        "        expected_unit: str,\n",
        "        retrieval_results: List[Dict],\n",
        "        question_type: str,\n",
        "        is_multi_doc: bool\n",
        "    ) -> str:\n",
        "        \"\"\"構建 User Prompt\"\"\"\n",
        "\n",
        "        available_doc_ids = list(set([r['doc_id'] for r in retrieval_results]))\n",
        "        doc_id_list = \", \".join(available_doc_ids)\n",
        "\n",
        "        # 問題類型提示\n",
        "        type_hints = {\n",
        "            \"table_extraction\": \"⚠️ HINT: Answer is in a TABLE. Search all rows carefully!\",\n",
        "            \"model_specific\": \"⚠️ HINT: Look for this specific MODEL's data in the context.\",\n",
        "            \"score_extraction\": \"⚠️ HINT: Look for benchmark scores or performance metrics.\",\n",
        "            \"term_extraction\": \"⚠️ HINT: Look for technical terms, tool names, or definitions.\",\n",
        "            \"numeric_extraction\": \"⚠️ HINT: Look for specific numbers or percentages.\",\n",
        "        }\n",
        "        hint = type_hints.get(question_type, \"\")\n",
        "\n",
        "        multi_hint = \"\\n⚠️ May need to combine data from multiple documents.\" if is_multi_doc else \"\"\n",
        "\n",
        "        return f\"\"\"QUESTION:\n",
        "{question}\n",
        "\n",
        "{hint}{multi_hint}\n",
        "\n",
        "EXPECTED UNIT: {expected_unit}\n",
        "\n",
        "AVAILABLE DOC IDs: {doc_id_list}\n",
        "\n",
        "CONTEXT:\n",
        "{context[:12000]}\n",
        "\n",
        "OUTPUT FORMAT (STRICT JSON):\n",
        "{{\n",
        "  \"answer\": \"natural language answer with unit\",\n",
        "  \"answer_value\": \"number or keyword (NO unit, NO symbols like <, >, ~)\",\n",
        "  \"answer_unit\": \"{expected_unit}\",\n",
        "  \"supporting_text\": \"verbatim quote from context (≤30 words)\",\n",
        "  \"explanation\": \"step-by-step reasoning (mention row/col for tables)\",\n",
        "  \"source_doc_ids\": [\"doc_id\"]\n",
        "}}\n",
        "\n",
        "SPECIAL CASES:\n",
        "- TRUE/FALSE: answer_value = \"1\" (TRUE) or \"0\" (FALSE)\n",
        "- Range: answer_value = \"[low,high]\" (no spaces inside brackets)\n",
        "- Unable to answer: ONLY if you searched ALL context and found NOTHING relevant\n",
        "\n",
        "GENERATE JSON NOW:\"\"\"\n",
        "\n",
        "    def _parse_response(self, content: str, expected_unit: str) -> Dict:\n",
        "        \"\"\"解析 Gemini 響應\"\"\"\n",
        "        try:\n",
        "            content = content.replace('```json', '').replace('```', '').strip()\n",
        "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "            if not json_match:\n",
        "                return self._default_answer(expected_unit)\n",
        "\n",
        "            result = json.loads(json_match.group())\n",
        "\n",
        "            # 確保必要欄位\n",
        "            required_fields = ['answer', 'answer_value', 'answer_unit',\n",
        "                             'supporting_text', 'explanation', 'source_doc_ids']\n",
        "            for field in required_fields:\n",
        "                if field not in result or result[field] is None:\n",
        "                    if field == 'source_doc_ids':\n",
        "                        result[field] = []\n",
        "                    else:\n",
        "                        result[field] = 'is_blank'\n",
        "\n",
        "            # 處理 source_doc_ids\n",
        "            doc_ids = result.get('source_doc_ids', [])\n",
        "            if isinstance(doc_ids, str):\n",
        "                if doc_ids == 'is_blank' or not doc_ids:\n",
        "                    result['source_doc_ids'] = []\n",
        "                else:\n",
        "                    result['source_doc_ids'] = [doc_ids]\n",
        "            elif not isinstance(doc_ids, list):\n",
        "                result['source_doc_ids'] = []\n",
        "\n",
        "            result['source_doc_ids'] = list(set(result['source_doc_ids']))\n",
        "\n",
        "            # ✅ 修正：不區分大小寫檢查 \"unable to answer\"\n",
        "            answer_text = result.get('answer', '').lower()\n",
        "            if 'unable to answer' in answer_text or 'cannot answer' in answer_text or 'not able to answer' in answer_text:\n",
        "                return {\n",
        "                    'answer': 'Unable to answer with confidence based on the provided documents.',\n",
        "                    'answer_value': 'is_blank',\n",
        "                    'answer_unit': expected_unit,\n",
        "                    'supporting_text': 'is_blank',\n",
        "                    'explanation': 'is_blank',\n",
        "                    'source_doc_ids': []\n",
        "                }\n",
        "\n",
        "            # TRUE/FALSE 處理\n",
        "            answer_str = str(result.get('answer', '')).strip()\n",
        "            answer_val = str(result.get('answer_value', '')).strip().upper()\n",
        "\n",
        "            # 檢查多種 TRUE/FALSE 格式\n",
        "            if answer_str.upper() in ['TRUE', 'FALSE'] or answer_val in ['TRUE', 'FALSE']:\n",
        "                is_true = answer_str.upper() == 'TRUE' or answer_val == 'TRUE'\n",
        "                result['answer'] = 'TRUE' if is_true else 'FALSE'\n",
        "                result['answer_value'] = '1' if is_true else '0'\n",
        "                result['answer_unit'] = 'is_blank'\n",
        "            else:\n",
        "                result['answer_unit'] = expected_unit\n",
        "\n",
        "            # 清理空值\n",
        "            for key in result:\n",
        "                if result[key] is None or result[key] == 'null':\n",
        "                    if key == 'source_doc_ids':\n",
        "                        result[key] = []\n",
        "                    else:\n",
        "                        result[key] = 'is_blank'\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  解析錯誤: {e}\")\n",
        "            return self._default_answer(expected_unit)\n",
        "\n",
        "    def _default_answer(self, expected_unit: str) -> Dict:\n",
        "        \"\"\"默認答案\"\"\"\n",
        "        return {\n",
        "            'answer': \"Unable to answer with confidence based on the provided documents.\",\n",
        "            'answer_value': 'is_blank',\n",
        "            'answer_unit': expected_unit,\n",
        "            'supporting_text': 'is_blank',\n",
        "            'explanation': 'is_blank',\n",
        "            'source_doc_ids': []\n",
        "        }\n",
        "\n",
        "    def print_stats(self):\n",
        "        \"\"\"打印統計\"\"\"\n",
        "        print(f\"\\n📊 GPT 統計:\")\n",
        "        print(f\"  模型: {self.model}\")\n",
        "        print(f\"  總調用: {self.call_count}\")\n",
        "        print(f\"  Stage 1 成功: {self.stage1_success}\")\n",
        "        print(f\"  Stage 2 重試: {self.stage2_retry}\")\n",
        "        print(f\"  激進模式使用: {self.aggressive_count}\")\n",
        "        print(f\"  多文檔問題: {self.multi_doc_count}\")\n",
        "        print(f\"  無文本回應: {self.no_text_count}\")\n",
        "        print(f\"  總成本: ${self.total_cost:.4f} USD\")\n",
        "        if self.call_count > 0:\n",
        "            print(f\"  平均成本: ${self.total_cost/self.call_count:.4f} USD\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. 提交文件生成　（沒改）\n",
        "# ============================================================================\n",
        "\n",
        "def generate_submission(\n",
        "    test_df: pd.DataFrame,\n",
        "    bm25: BM25Okapi,\n",
        "    chunks: List[Dict],\n",
        "    metadata_df: pd.DataFrame,\n",
        "    generator: EnhancedTwoStageGPTGenerator,\n",
        "    output_path: str = \"submission_enhanced.csv\"\n",
        "):\n",
        "    \"\"\"生成提交文件\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"🚀 開始生成提交文件 (增強版 - Gemini)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    submissions = []\n",
        "\n",
        "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"生成中\"):\n",
        "        qid = row['id']\n",
        "        question = row['question']\n",
        "        expected_unit = row.get('answer_unit', 'is_blank')\n",
        "        if pd.isna(expected_unit):\n",
        "            expected_unit = 'is_blank'\n",
        "\n",
        "        # BM25 檢索 Top-10\n",
        "        retrieval_results = search_bm25(question, bm25, chunks, top_k=10)\n",
        "\n",
        "        if not retrieval_results:\n",
        "            submission_row = {\n",
        "                'id': qid,\n",
        "                'question': question,\n",
        "                'answer': 'Unable to answer with confidence based on the provided documents.',\n",
        "                'answer_value': 'is_blank',\n",
        "                'answer_unit': expected_unit,\n",
        "                'ref_id': 'is_blank',\n",
        "                'ref_url': 'is_blank',\n",
        "                'supporting_materials': 'is_blank',\n",
        "                'explanation': 'is_blank'\n",
        "            }\n",
        "        else:\n",
        "            # 生成答案\n",
        "            answer_dict = generator.generate_answer(question, retrieval_results, expected_unit)\n",
        "\n",
        "            # 處理 ref_id\n",
        "            is_unable = 'Unable to answer' in answer_dict.get('answer', '')\n",
        "\n",
        "            if is_unable:\n",
        "                ref_id_value = 'is_blank'\n",
        "                ref_url_value = 'is_blank'\n",
        "            else:\n",
        "                source_doc_ids = answer_dict.get('source_doc_ids', [])\n",
        "\n",
        "                # 驗證並過濾有效的 doc_ids\n",
        "                valid_doc_ids = []\n",
        "                valid_urls = []\n",
        "\n",
        "                for doc_id in source_doc_ids:\n",
        "                    if doc_id and doc_id != 'is_blank':\n",
        "                        mask = metadata_df['id'] == doc_id\n",
        "                        if mask.any():\n",
        "                            valid_doc_ids.append(doc_id)\n",
        "                            valid_urls.append(metadata_df[mask]['url'].values[0])\n",
        "\n",
        "                # 後備: 如果沒有有效 doc_id，使用 Top-1\n",
        "                if not valid_doc_ids:\n",
        "                    fallback_id = retrieval_results[0]['doc_id']\n",
        "                    mask = metadata_df['id'] == fallback_id\n",
        "                    if mask.any():\n",
        "                        valid_doc_ids = [fallback_id]\n",
        "                        valid_urls = [metadata_df[mask]['url'].values[0]]\n",
        "\n",
        "                # 格式化\n",
        "                if valid_doc_ids:\n",
        "                    ref_id_value = str(valid_doc_ids).replace('\"', \"'\")\n",
        "                    ref_url_value = str(valid_urls).replace('\"', \"'\")\n",
        "                else:\n",
        "                    ref_id_value = 'is_blank'\n",
        "                    ref_url_value = 'is_blank'\n",
        "\n",
        "            submission_row = {\n",
        "                'id': qid,\n",
        "                'question': question,\n",
        "                'answer': answer_dict.get('answer', 'is_blank'),\n",
        "                'answer_value': answer_dict.get('answer_value', 'is_blank'),\n",
        "                'answer_unit': expected_unit,\n",
        "                'ref_id': ref_id_value,\n",
        "                'ref_url': ref_url_value,\n",
        "                'supporting_materials': answer_dict.get('supporting_text', 'is_blank'),\n",
        "                'explanation': answer_dict.get('explanation', 'is_blank')\n",
        "            }\n",
        "\n",
        "        submissions.append(submission_row)\n",
        "\n",
        "    submission_df = pd.DataFrame(submissions)\n",
        "\n",
        "    # 驗證\n",
        "    print(\"\\n✅ 驗證提交文件...\")\n",
        "    if submission_df.isnull().any().any():\n",
        "        print(\"⚠️ 警告: 存在 NULL 值，正在修復...\")\n",
        "        submission_df = submission_df.fillna('is_blank')\n",
        "    else:\n",
        "        print(\"✓ 沒有 NULL 值\")\n",
        "\n",
        "    # 統計\n",
        "    unable_count = sum(submission_df['answer'].str.contains('Unable to answer', na=False))\n",
        "    multi_ref_count = sum(submission_df['ref_id'].str.count(',') >= 1)\n",
        "\n",
        "    print(f\"✓ Unable to answer: {unable_count} 題\")\n",
        "    print(f\"✓ 多文檔引用: {multi_ref_count} 題\")\n",
        "    print(f\"✓ 總題數: {len(submission_df)}\")\n",
        "\n",
        "    submission_df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "    print(f\"\\n📄 提交文件已保存: {output_path}\")\n",
        "\n",
        "    generator.print_stats()\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. 數據加載輔助函數（沒改）\n",
        "# ============================================================================\n",
        "\n",
        "def load_hybrid_chunks(chunks_file: str) -> List[Dict]:\n",
        "    \"\"\"加載 hybrid_chunks\"\"\"\n",
        "    print(\"📂 載入 hybrid_chunks...\")\n",
        "\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        chunks = json.load(f)\n",
        "\n",
        "    print(f\"✓ 完成: {len(chunks)} chunks\")\n",
        "    print(f\"  - 文本: {sum(1 for c in chunks if c.get('type') == 'text')}\")\n",
        "    print(f\"  - 表格: {sum(1 for c in chunks if c.get('type') == 'table')}\")\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. 主函數（僅改成Gemini)\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"主執行函數\"\"\"\n",
        "\n",
        "    # ⚠️ 請根據你的環境修改這些路徑\n",
        "    INPUT_DIR = Path(\"/root/.cache/kagglehub/competitions/WattBot2025\")\n",
        "    CHUNKS_FILE = \"/content/hybrid_chunks.json\"\n",
        "\n",
        "    # 選擇模型\n",
        "    SELECTED_MODEL = \"gemini-2.5-pro\"  # 或 \"gemini-2.5-flash\" / \"gemini-3-pro-preview\"\n",
        "    OUTPUT_FILE = f\"/content/submission_{SELECTED_MODEL.replace('-', '_')}.csv\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"📂 載入數據...\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    chunks = load_hybrid_chunks(CHUNKS_FILE)\n",
        "\n",
        "    metadata = pd.read_csv(INPUT_DIR / \"metadata.csv\", encoding=\"latin-1\")\n",
        "    test_q = pd.read_csv(INPUT_DIR / \"test_Q.csv\", encoding=\"latin-1\")\n",
        "\n",
        "    metadata.columns = [col.replace('\\ufeff', '').replace('ï»¿', '') for col in metadata.columns]\n",
        "    test_q.columns = [col.replace('\\ufeff', '').replace('ï»¿', '') for col in test_q.columns]\n",
        "\n",
        "    print(f\"✓ Metadata: {len(metadata)} 篇\")\n",
        "    print(f\"✓ Test questions: {len(test_q)} 題\")\n",
        "\n",
        "    bm25 = build_bm25_index(chunks)\n",
        "\n",
        "    print(\"\\n🤖 初始化增強版生成器...\")\n",
        "    generator = EnhancedTwoStageGPTGenerator(\n",
        "        model=SELECTED_MODEL,\n",
        "        client=client\n",
        "    )\n",
        "    print(\"✓ 完成\")\n",
        "\n",
        "    submission_df = generate_submission(\n",
        "        test_df=test_q,\n",
        "        bm25=bm25,\n",
        "        chunks=chunks,\n",
        "        metadata_df=metadata,\n",
        "        generator=generator,\n",
        "        output_path=OUTPUT_FILE\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✅ 完成!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 執行\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871,
          "referenced_widgets": [
            "228ed790a97744d7b974031b41e9f54a",
            "98a68eb829a94aa8bacfe74485c3b3cd",
            "4b20b5526255448991ea10bba03007f6",
            "e1c9a26e5fe74199bd7a25408e27d000",
            "36cf7929b17542d69be4a6554cff083d",
            "11b3470c69b34df8ae1d08d0f1de08a6",
            "ac326d8afaec4caab9e6cf187dccdc07",
            "4202c542e45747f9bca70167c8587cd2",
            "f3c0dda798f3402ea81f3b576c1c748e",
            "16f66c94bc604c0a8d9e9c0c5c619818",
            "d22dd860cb164085a04ceeac2a963518"
          ]
        },
        "id": "4UouLzOVoBY-",
        "outputId": "9dc35e57-0b0b-48d2-fb78-712a86d300a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "📂 載入數據...\n",
            "======================================================================\n",
            "📂 載入 hybrid_chunks...\n",
            "✓ 完成: 2608 chunks\n",
            "  - 文本: 2372\n",
            "  - 表格: 134\n",
            "✓ Metadata: 32 篇\n",
            "✓ Test questions: 282 題\n",
            "\n",
            "🔨 建立 BM25 索引...\n",
            "✓ BM25 索引完成 (2608 chunks)\n",
            "\n",
            "🤖 初始化增強版生成器...\n",
            "📌 使用模型: gemini-2.5-pro\n",
            "   Gemini 2.5 Pro - 高品質 (RPM 150)\n",
            "   定價: $1.25/$5.0 per 1M tokens\n",
            "   Rate Limit: 150/min\n",
            "✓ 完成\n",
            "======================================================================\n",
            "🚀 開始生成提交文件 (增強版 - Gemini)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "生成中:   0%|          | 0/282 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "228ed790a97744d7b974031b41e9f54a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 驗證提交文件...\n",
            "✓ 沒有 NULL 值\n",
            "✓ Unable to answer: 28 題\n",
            "✓ 多文檔引用: 12 題\n",
            "✓ 總題數: 282\n",
            "\n",
            "📄 提交文件已保存: /content/submission_gemini_2.5_pro.csv\n",
            "\n",
            "📊 GPT 統計:\n",
            "  模型: gemini-2.5-pro\n",
            "  總調用: 332\n",
            "  Stage 1 成功: 232\n",
            "  Stage 2 重試: 50\n",
            "  激進模式使用: 178\n",
            "  多文檔問題: 10\n",
            "  無文本回應: 0\n",
            "  總成本: $1.2287 USD\n",
            "  平均成本: $0.0037 USD\n",
            "\n",
            "======================================================================\n",
            "✅ 完成!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0.743\n",
        "# \"\"\"\n",
        "# WattBot 2025 - 保守多文檔支持版本\n",
        "# ==================================\n",
        "# 基於原始代碼修改，關鍵改進:\n",
        "# 1. 保持 Unable to answer 機制 (不激進)\n",
        "# 2. 只在真正需要時才多文檔引用\n",
        "# 3. Stage 2 relaxed mode 保持保守\n",
        "# \"\"\"\n",
        "\n",
        "# import os\n",
        "# import re\n",
        "# import json\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import openai\n",
        "# from pathlib import Path\n",
        "# from typing import List, Dict, Optional, Tuple\n",
        "# from tqdm.auto import tqdm\n",
        "# from rank_bm25 import BM25Okapi\n",
        "# from collections import defaultdict\n",
        "\n",
        "# OPENAI_API_KEY = \"# ⚠️ 請替換成你的 API Key\"  # 請替換\n",
        "# openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # 1. 問題預分類器 - 保守版本\n",
        "# # ============================================================================\n",
        "\n",
        "# def detect_multi_doc_question(question: str) -> Tuple[bool, str]:\n",
        "#     \"\"\"\n",
        "#     保守地偵測是否需要多篇論文\n",
        "#     只有非常明確的情況才返回 True\n",
        "#     \"\"\"\n",
        "#     question_lower = question.lower()\n",
        "\n",
        "#     # 只有這些非常明確的模式才認為需要多文檔\n",
        "#     # 類型 1: 明確需要跨文檔計算 (需要兩個不同來源的數據)\n",
        "#     cross_calc_patterns = [\n",
        "#         r'equivalent to .+ (household|american|person|life)',  # 需要換算基準\n",
        "#         r'how many .+ equivalent to',\n",
        "#     ]\n",
        "#     for pattern in cross_calc_patterns:\n",
        "#         if re.search(pattern, question_lower):\n",
        "#             return True, \"cross_doc_calculation\"\n",
        "\n",
        "#     # 類型 2: 明確提到要結合多個來源\n",
        "#     if 'and how does this compare' in question_lower:\n",
        "#         return True, \"explicit_comparison\"\n",
        "\n",
        "#     return False, \"single_doc\"\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # 2. 數據加載模組\n",
        "# # ============================================================================\n",
        "\n",
        "# def load_hybrid_chunks(chunks_file: str) -> List[Dict]:\n",
        "#     \"\"\"加載 hybrid_chunks\"\"\"\n",
        "#     print(\"📂 載入 hybrid_chunks...\")\n",
        "\n",
        "#     with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "#         chunks = json.load(f)\n",
        "\n",
        "#     print(f\"✓ 完成: {len(chunks)} chunks\")\n",
        "#     print(f\"  - 文本: {sum(1 for c in chunks if c['type'] == 'text')}\")\n",
        "#     print(f\"  - 表格: {sum(1 for c in chunks if c['type'] == 'table')}\")\n",
        "\n",
        "#     return chunks\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # 3. BM25 檢索模組\n",
        "# # ============================================================================\n",
        "\n",
        "# def enhanced_tokenize(text: str) -> List[str]:\n",
        "#     \"\"\"增強分詞器\"\"\"\n",
        "#     text = text.lower()\n",
        "#     tokens = []\n",
        "\n",
        "#     tokens.extend(re.findall(r'\\d+\\.?\\d*%?', text))\n",
        "#     tokens.extend(re.findall(r'\\b20\\d{2}\\b', text))\n",
        "#     tokens.extend(re.findall(\n",
        "#         r'\\b(?:gpu|tpu|cpu|kwh|mwh|twh|co2|tco2e|gflops|teraflops|'\n",
        "#         r'bert|gpt|llama|transformer|household|consumption)\\b', text\n",
        "#     ))\n",
        "\n",
        "#     words = re.findall(r'[a-z]+', text)\n",
        "#     tokens.extend(words)\n",
        "#     tokens.extend([f\"{words[i]}_{words[i+1]}\" for i in range(min(5, len(words)-1))])\n",
        "\n",
        "#     return tokens\n",
        "\n",
        "\n",
        "# def build_bm25_index(chunks: List[Dict]) -> BM25Okapi:\n",
        "#     \"\"\"建立 BM25 索引\"\"\"\n",
        "#     print(\"\\n🔨 建立 BM25 索引...\")\n",
        "#     corpus = [chunk['content'] for chunk in chunks]\n",
        "#     tokenized_corpus = [enhanced_tokenize(text) for text in corpus]\n",
        "#     bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "#     print(f\"✓ BM25 索引完成 ({len(corpus)} chunks)\")\n",
        "#     return bm25\n",
        "\n",
        "\n",
        "# def search_bm25(query: str, bm25: BM25Okapi, chunks: List[Dict], top_k: int = 10) -> List[Dict]:\n",
        "#     \"\"\"BM25 檢索\"\"\"\n",
        "#     tokenized_query = enhanced_tokenize(query)\n",
        "#     scores = bm25.get_scores(tokenized_query)\n",
        "#     top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "#     results = []\n",
        "#     for rank, idx in enumerate(top_indices, 1):\n",
        "#         chunk = chunks[idx]\n",
        "#         results.append({\n",
        "#             'rank': rank,\n",
        "#             'score': float(scores[idx]),\n",
        "#             'doc_id': chunk['doc_id'],\n",
        "#             'content': chunk['content'],\n",
        "#             'type': chunk['type']\n",
        "#         })\n",
        "\n",
        "#     return results\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # 4. 兩階段生成器 - 保守多文檔版\n",
        "# # ============================================================================\n",
        "\n",
        "# class TwoStageGPTGenerator:\n",
        "#     \"\"\"\n",
        "#     兩階段生成策略 + 保守多文檔支持\n",
        "\n",
        "#     關鍵原則:\n",
        "#     1. 保持 Unable to answer 機制\n",
        "#     2. 只在明確需要時才多文檔引用\n",
        "#     3. Stage 2 不強制回答\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "#         self.model = model\n",
        "#         self.call_count = 0\n",
        "#         self.total_cost = 0.0\n",
        "#         self.stage1_success = 0\n",
        "#         self.stage2_retry = 0\n",
        "#         self.multi_doc_count = 0\n",
        "\n",
        "#     def generate_answer(\n",
        "#         self,\n",
        "#         question: str,\n",
        "#         retrieval_results: List[Dict],\n",
        "#         expected_unit: str = \"is_blank\"\n",
        "#     ) -> Dict:\n",
        "#         \"\"\"兩階段生成主函數\"\"\"\n",
        "\n",
        "#         # 問題預分類\n",
        "#         is_multi_doc, reason = detect_multi_doc_question(question)\n",
        "#         if is_multi_doc:\n",
        "#             self.multi_doc_count += 1\n",
        "\n",
        "#         # ============================================\n",
        "#         # Stage 1: Top-3 + 嚴格模式\n",
        "#         # ============================================\n",
        "#         top3_results = retrieval_results[:3]\n",
        "#         context_top3 = self._build_context(top3_results)\n",
        "\n",
        "#         stage1_response = self._call_gpt(\n",
        "#             question=question,\n",
        "#             context=context_top3,\n",
        "#             retrieval_results=top3_results,\n",
        "#             expected_unit=expected_unit,\n",
        "#             mode=\"strict\",\n",
        "#             is_multi_doc=is_multi_doc\n",
        "#         )\n",
        "\n",
        "#         is_unable = \"Unable to answer\" in stage1_response.get('answer', '')\n",
        "\n",
        "#         if not is_unable:\n",
        "#             self.stage1_success += 1\n",
        "#             return stage1_response\n",
        "\n",
        "#         # ============================================\n",
        "#         # Stage 2: Top-5 + 保守寬鬆模式 (Fallback)\n",
        "#         # ============================================\n",
        "#         self.stage2_retry += 1\n",
        "#         top5_results = retrieval_results[:10]\n",
        "#         context_top5 = self._build_context(top5_results)\n",
        "\n",
        "#         stage2_response = self._call_gpt(\n",
        "#             question=question,\n",
        "#             context=context_top5,\n",
        "#             retrieval_results=top5_results,\n",
        "#             expected_unit=expected_unit,\n",
        "#             mode=\"relaxed\",\n",
        "#             is_multi_doc=is_multi_doc\n",
        "#         )\n",
        "\n",
        "#         return stage2_response\n",
        "\n",
        "#     def _build_context(self, results: List[Dict]) -> str:\n",
        "#         \"\"\"構建上下文\"\"\"\n",
        "#         context_parts = []\n",
        "\n",
        "#         for i, r in enumerate(results, 1):\n",
        "#             content = r['content']\n",
        "#             chunk_type = r['type']\n",
        "#             doc_id = r['doc_id']\n",
        "\n",
        "#             if chunk_type == 'table':\n",
        "#                 prefix = (\n",
        "#                     f\"[DOCUMENT #{i}: {doc_id}]\\n\"\n",
        "#                     f\"⚠️ WARNING: DATA TABLE - READ ROW/COL CAREFULLY:\\n\"\n",
        "#                     f\"{'='*70}\\n\"\n",
        "#                 )\n",
        "#             else:\n",
        "#                 prefix = f\"[DOCUMENT #{i}: {doc_id}]\\n\"\n",
        "\n",
        "#             context_parts.append(prefix + content)\n",
        "\n",
        "#         return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "#     def _call_gpt(\n",
        "#         self,\n",
        "#         question: str,\n",
        "#         context: str,\n",
        "#         retrieval_results: List[Dict],\n",
        "#         expected_unit: str,\n",
        "#         mode: str,\n",
        "#         is_multi_doc: bool\n",
        "#     ) -> Dict:\n",
        "#         \"\"\"調用 GPT API\"\"\"\n",
        "\n",
        "#         if mode == \"strict\":\n",
        "#             system_prompt = self._get_strict_system_prompt(is_multi_doc)\n",
        "#         else:\n",
        "#             system_prompt = self._get_relaxed_system_prompt(is_multi_doc)\n",
        "\n",
        "#         user_prompt = self._build_user_prompt(\n",
        "#             question, context, expected_unit, retrieval_results, is_multi_doc\n",
        "#         )\n",
        "\n",
        "#         try:\n",
        "#             response = openai.chat.completions.create(\n",
        "#                 model=self.model,\n",
        "#                 messages=[\n",
        "#                     {\"role\": \"system\", \"content\": system_prompt},\n",
        "#                     {\"role\": \"user\", \"content\": user_prompt}\n",
        "#                 ],\n",
        "#                 temperature=0 if mode == \"strict\" else 0.3,  # ⭐ 保持低溫度\n",
        "#                 max_tokens=1500\n",
        "#             )\n",
        "\n",
        "#             self.call_count += 1\n",
        "#             content = response.choices[0].message.content\n",
        "#             result = self._parse_response(content, expected_unit)\n",
        "\n",
        "#             # 計算成本\n",
        "#             input_tokens = response.usage.prompt_tokens\n",
        "#             output_tokens = response.usage.completion_tokens\n",
        "#             cost = (input_tokens * 0.00015 + output_tokens * 0.0006) / 1000\n",
        "#             self.total_cost += cost\n",
        "\n",
        "#             return result\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"  ❌ API 錯誤: {e}\")\n",
        "#             return self._default_answer(expected_unit)\n",
        "\n",
        "#     def _get_strict_system_prompt(self, is_multi_doc: bool) -> str:\n",
        "#         \"\"\"嚴格模式 System Prompt\"\"\"\n",
        "\n",
        "#         # 多文檔指引 (只在需要時加入)\n",
        "#         multi_doc_section = \"\"\"\n",
        "# ⭐ MULTI-DOCUMENT CITATION (when needed):\n",
        "# - This question may require combining information from multiple documents.\n",
        "# - If you need data from multiple sources, list them in \"source_doc_ids\" as a list: [\"doc1\", \"doc2\"]\n",
        "# - In \"explanation\", show which data came from which document.\n",
        "# \"\"\" if is_multi_doc else \"\"\n",
        "\n",
        "#         return f\"\"\"⚠️ CRITICAL JSON OUTPUT RULES:\n",
        "# - The output MUST be a single valid JSON object.\n",
        "# - ALL fields are REQUIRED: answer, answer_value, answer_unit, supporting_text, explanation, source_doc_ids\n",
        "# - NEVER use null or None. Use \"is_blank\" for empty values.\n",
        "# - \"source_doc_ids\" MUST be a LIST: [\"doc_id\"] for single, [\"doc1\", \"doc2\"] for multiple.\n",
        "\n",
        "# ⚠️ UNABLE TO ANSWER RULES:\n",
        "# - If the question CANNOT be answered based on the provided documents, you MUST respond:\n",
        "#   answer: \"Unable to answer with confidence based on the provided documents.\"\n",
        "#   answer_value: \"is_blank\"\n",
        "#   source_doc_ids: []\n",
        "# - DO NOT guess or make up information.\n",
        "# - DO NOT use information not present in the context.\n",
        "# {multi_doc_section}\n",
        "# ⭐ SOURCE DOCUMENT ATTRIBUTION:\n",
        "# - Identify which document(s) you used.\n",
        "# - Output as a LIST: [\"doc_id\"]\n",
        "# - If unable to answer: []\n",
        "\n",
        "# 📊 TABLE DATA RULES:\n",
        "# - Tables have Headers and Data rows separated by \"|\"\n",
        "# - ALWAYS read row and column labels carefully.\n",
        "# - DO NOT confuse rows with columns.\n",
        "\n",
        "# 🔢 UNIT CONVERSION RULES:\n",
        "# - MWh → kWh: multiply by 1000\n",
        "# - tCO2e → kg: multiply by 1000\n",
        "# - million → raw: multiply by 1,000,000\n",
        "# - ALWAYS CHECK if unit conversion is needed.\n",
        "\n",
        "# 🧠 CHAIN OF THOUGHT:\n",
        "# - Show step-by-step reasoning in \"explanation\".\n",
        "# - For calculations: Show formula and computation.\n",
        "\n",
        "# ✅ STRICT MODE: Answer ONLY if evidence is EXPLICIT and CLEAR in the documents.\n",
        "\n",
        "# You are a scientific evidence extraction expert. Be precise and conservative.\"\"\"\n",
        "\n",
        "#     def _get_relaxed_system_prompt(self, is_multi_doc: bool) -> str:\n",
        "#         \"\"\"寬鬆模式 System Prompt - ⭐ 但仍保守\"\"\"\n",
        "\n",
        "#         multi_doc_section = \"\"\"\n",
        "# ⭐ MULTI-DOCUMENT CITATION (when needed):\n",
        "# - If you need data from multiple sources, list them in \"source_doc_ids\": [\"doc1\", \"doc2\"]\n",
        "# \"\"\" if is_multi_doc else \"\"\n",
        "\n",
        "#         return f\"\"\"⚠️ CRITICAL JSON OUTPUT RULES:\n",
        "# - The output MUST be a single valid JSON object.\n",
        "# - ALL fields are REQUIRED: answer, answer_value, answer_unit, supporting_text, explanation, source_doc_ids\n",
        "# - NEVER use null or None. Use \"is_blank\" for empty values.\n",
        "# - \"source_doc_ids\" MUST be a LIST.\n",
        "\n",
        "# ⚠️ UNABLE TO ANSWER - STILL REQUIRED:\n",
        "# - If the question CANNOT be answered, you MUST respond:\n",
        "#   answer: \"Unable to answer with confidence based on the provided documents.\"\n",
        "#   answer_value: \"is_blank\"\n",
        "#   source_doc_ids: []\n",
        "# - Even in relaxed mode, DO NOT fabricate information.\n",
        "# {multi_doc_section}\n",
        "# 📊 TABLE DATA RULES:\n",
        "# - Read tables carefully. Match headers with data.\n",
        "\n",
        "# 🔢 UNIT CONVERSION:\n",
        "# - MWh → kWh: ×1000\n",
        "# - million → raw: ×1,000,000\n",
        "\n",
        "# 🌟 RELAXED MODE:\n",
        "# - You may make REASONABLE inferences if evidence is slightly indirect.\n",
        "# - But if the information is truly not in the documents, still say \"Unable to answer\".\n",
        "# - DO NOT use outside knowledge not present in the context.\n",
        "\n",
        "# You are a scientific evidence extraction expert.\"\"\"\n",
        "\n",
        "#     def _build_user_prompt(\n",
        "#         self,\n",
        "#         question: str,\n",
        "#         context: str,\n",
        "#         expected_unit: str,\n",
        "#         retrieval_results: List[Dict],\n",
        "#         is_multi_doc: bool\n",
        "#     ) -> str:\n",
        "#         \"\"\"構建 User Prompt\"\"\"\n",
        "\n",
        "#         available_doc_ids = list(set([r['doc_id'] for r in retrieval_results]))\n",
        "#         doc_id_list = \", \".join(available_doc_ids)\n",
        "\n",
        "#         multi_doc_hint = \"\"\"\n",
        "# ⚠️ NOTE: This question may require combining data from multiple documents.\"\"\" if is_multi_doc else \"\"\n",
        "\n",
        "#         return f\"\"\"QUESTION:\n",
        "# {question}\n",
        "# {multi_doc_hint}\n",
        "\n",
        "# EXPECTED ANSWER UNIT:\n",
        "# {expected_unit}\n",
        "\n",
        "# AVAILABLE DOCUMENT IDs:\n",
        "# {doc_id_list}\n",
        "\n",
        "# RETRIEVED CONTEXT:\n",
        "# {context[:8000]}\n",
        "\n",
        "# OUTPUT FORMAT (STRICT JSON):\n",
        "# {{\n",
        "#   \"answer\": \"natural language answer with unit\",\n",
        "#   \"answer_value\": \"pure number or keyword (NO unit, NO symbols like <, >, ~)\",\n",
        "#   \"answer_unit\": \"{expected_unit}\",\n",
        "#   \"supporting_text\": \"verbatim quote from context (≤25 words)\",\n",
        "#   \"explanation\": \"step-by-step reasoning\",\n",
        "#   \"source_doc_ids\": [\"doc_id\"]  // LIST format\n",
        "# }}\n",
        "\n",
        "# SPECIAL CASES:\n",
        "# - TRUE/FALSE: answer_value = \"1\" (TRUE) or \"0\" (FALSE)\n",
        "# - Range: answer_value = \"[low,high]\" (no spaces)\n",
        "# - Unable to answer: answer = \"Unable to answer with confidence based on the provided documents.\", answer_value = \"is_blank\", source_doc_ids = []\n",
        "\n",
        "# NOW GENERATE THE JSON:\"\"\"\n",
        "\n",
        "#     def _parse_response(self, content: str, expected_unit: str) -> Dict:\n",
        "#         \"\"\"解析 GPT 響應\"\"\"\n",
        "#         try:\n",
        "#             content = content.replace('```json', '').replace('```', '').strip()\n",
        "#             json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "#             if not json_match:\n",
        "#                 return self._default_answer(expected_unit)\n",
        "\n",
        "#             result = json.loads(json_match.group())\n",
        "\n",
        "#             # 確保必要欄位\n",
        "#             required_fields = ['answer', 'answer_value', 'answer_unit',\n",
        "#                              'supporting_text', 'explanation', 'source_doc_ids']\n",
        "#             for field in required_fields:\n",
        "#                 if field not in result or result[field] is None:\n",
        "#                     if field == 'source_doc_ids':\n",
        "#                         result[field] = []\n",
        "#                     else:\n",
        "#                         result[field] = 'is_blank'\n",
        "\n",
        "#             # ⭐ 處理 source_doc_ids - 確保是 list\n",
        "#             doc_ids = result.get('source_doc_ids', [])\n",
        "#             if isinstance(doc_ids, str):\n",
        "#                 if doc_ids == 'is_blank' or not doc_ids:\n",
        "#                     result['source_doc_ids'] = []\n",
        "#                 else:\n",
        "#                     result['source_doc_ids'] = [doc_ids]\n",
        "#             elif not isinstance(doc_ids, list):\n",
        "#                 result['source_doc_ids'] = []\n",
        "\n",
        "#             # ⭐ 去重 (避免 ['doc', 'doc'] 這種情況)\n",
        "#             result['source_doc_ids'] = list(set(result['source_doc_ids']))\n",
        "\n",
        "#             # 處理 Unable to answer\n",
        "#             if 'Unable to answer' in result.get('answer', ''):\n",
        "#                 return {\n",
        "#                     'answer': 'Unable to answer with confidence based on the provided documents.',\n",
        "#                     'answer_value': 'is_blank',\n",
        "#                     'answer_unit': expected_unit,\n",
        "#                     'supporting_text': 'is_blank',\n",
        "#                     'explanation': 'is_blank',\n",
        "#                     'source_doc_ids': []\n",
        "#                 }\n",
        "\n",
        "#             # TRUE/FALSE 處理\n",
        "#             answer_str = str(result.get('answer', ''))\n",
        "#             if answer_str.upper() in ['TRUE', 'FALSE']:\n",
        "#                 result['answer'] = answer_str.upper()\n",
        "#                 result['answer_value'] = '1' if answer_str.upper() == 'TRUE' else '0'\n",
        "#                 result['answer_unit'] = 'is_blank'\n",
        "\n",
        "#             result['answer_unit'] = expected_unit\n",
        "\n",
        "#             # 清理空值\n",
        "#             for key in result:\n",
        "#                 if result[key] is None or result[key] == 'null':\n",
        "#                     if key == 'source_doc_ids':\n",
        "#                         result[key] = []\n",
        "#                     else:\n",
        "#                         result[key] = 'is_blank'\n",
        "\n",
        "#             return result\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"  解析錯誤: {e}\")\n",
        "#             return self._default_answer(expected_unit)\n",
        "\n",
        "#     def _default_answer(self, expected_unit: str) -> Dict:\n",
        "#         \"\"\"默認答案\"\"\"\n",
        "#         return {\n",
        "#             'answer': \"Unable to answer with confidence based on the provided documents.\",\n",
        "#             'answer_value': 'is_blank',\n",
        "#             'answer_unit': expected_unit,\n",
        "#             'supporting_text': 'is_blank',\n",
        "#             'explanation': 'is_blank',\n",
        "#             'source_doc_ids': []\n",
        "#         }\n",
        "\n",
        "#     def print_stats(self):\n",
        "#         \"\"\"打印統計\"\"\"\n",
        "#         print(f\"\\n📊 GPT 統計:\")\n",
        "#         print(f\"  總調用: {self.call_count}\")\n",
        "#         print(f\"  Stage 1 成功: {self.stage1_success}\")\n",
        "#         print(f\"  Stage 2 重試: {self.stage2_retry}\")\n",
        "#         print(f\"  多文檔問題偵測: {self.multi_doc_count}\")\n",
        "#         print(f\"  總成本: ${self.total_cost:.4f} USD\")\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # 5. 提交生成主函數\n",
        "# # ============================================================================\n",
        "\n",
        "# def generate_submission(\n",
        "#     test_df: pd.DataFrame,\n",
        "#     bm25: BM25Okapi,\n",
        "#     chunks: List[Dict],\n",
        "#     metadata_df: pd.DataFrame,\n",
        "#     generator: TwoStageGPTGenerator,\n",
        "#     output_path: str = \"/content/submission_conservative.csv\"\n",
        "# ):\n",
        "#     \"\"\"生成提交文件\"\"\"\n",
        "\n",
        "#     print(\"=\"*70)\n",
        "#     print(\"🚀 開始生成提交文件 (保守多文檔版)\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     submissions = []\n",
        "\n",
        "#     for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"生成中\"):\n",
        "#         qid = row['id']\n",
        "#         question = row['question']\n",
        "#         expected_unit = row.get('answer_unit', 'is_blank')\n",
        "#         if pd.isna(expected_unit):\n",
        "#             expected_unit = 'is_blank'\n",
        "\n",
        "#         # BM25 檢索 Top-10\n",
        "#         retrieval_results = search_bm25(question, bm25, chunks, top_k=10)\n",
        "\n",
        "#         if not retrieval_results:\n",
        "#             submission_row = {\n",
        "#                 'id': qid,\n",
        "#                 'question': question,\n",
        "#                 'answer': 'Unable to answer with confidence based on the provided documents.',\n",
        "#                 'answer_value': 'is_blank',\n",
        "#                 'answer_unit': expected_unit,\n",
        "#                 'ref_id': 'is_blank',\n",
        "#                 'ref_url': 'is_blank',\n",
        "#                 'supporting_materials': 'is_blank',\n",
        "#                 'explanation': 'is_blank'\n",
        "#             }\n",
        "#         else:\n",
        "#             # 生成答案\n",
        "#             answer_dict = generator.generate_answer(question, retrieval_results, expected_unit)\n",
        "\n",
        "#             # 處理 ref_id\n",
        "#             is_unable = 'Unable to answer' in answer_dict.get('answer', '')\n",
        "\n",
        "#             if is_unable:\n",
        "#                 ref_id_value = 'is_blank'\n",
        "#                 ref_url_value = 'is_blank'\n",
        "#             else:\n",
        "#                 source_doc_ids = answer_dict.get('source_doc_ids', [])\n",
        "\n",
        "#                 # 驗證並過濾有效的 doc_ids\n",
        "#                 valid_doc_ids = []\n",
        "#                 valid_urls = []\n",
        "\n",
        "#                 for doc_id in source_doc_ids:\n",
        "#                     if doc_id and doc_id != 'is_blank':\n",
        "#                         mask = metadata_df['id'] == doc_id\n",
        "#                         if mask.any():\n",
        "#                             valid_doc_ids.append(doc_id)\n",
        "#                             valid_urls.append(metadata_df[mask]['url'].values[0])\n",
        "\n",
        "#                 # 後備: 如果沒有有效 doc_id，使用 Top-1\n",
        "#                 if not valid_doc_ids:\n",
        "#                     fallback_id = retrieval_results[0]['doc_id']\n",
        "#                     mask = metadata_df['id'] == fallback_id\n",
        "#                     if mask.any():\n",
        "#                         valid_doc_ids = [fallback_id]\n",
        "#                         valid_urls = [metadata_df[mask]['url'].values[0]]\n",
        "\n",
        "#                 # 格式化\n",
        "#                 if valid_doc_ids:\n",
        "#                     ref_id_value = str(valid_doc_ids).replace('\"', \"'\")\n",
        "#                     ref_url_value = str(valid_urls).replace('\"', \"'\")\n",
        "#                 else:\n",
        "#                     ref_id_value = 'is_blank'\n",
        "#                     ref_url_value = 'is_blank'\n",
        "\n",
        "#             submission_row = {\n",
        "#                 'id': qid,\n",
        "#                 'question': question,\n",
        "#                 'answer': answer_dict.get('answer', 'is_blank'),\n",
        "#                 'answer_value': answer_dict.get('answer_value', 'is_blank'),\n",
        "#                 'answer_unit': expected_unit,\n",
        "#                 'ref_id': ref_id_value,\n",
        "#                 'ref_url': ref_url_value,\n",
        "#                 'supporting_materials': answer_dict.get('supporting_text', 'is_blank'),\n",
        "#                 'explanation': answer_dict.get('explanation', 'is_blank')\n",
        "#             }\n",
        "\n",
        "#         submissions.append(submission_row)\n",
        "\n",
        "#     submission_df = pd.DataFrame(submissions)\n",
        "\n",
        "#     # 驗證\n",
        "#     print(\"\\n✅ 驗證提交文件...\")\n",
        "#     if submission_df.isnull().any().any():\n",
        "#         print(\"⚠️ 警告: 存在 NULL 值\")\n",
        "#     else:\n",
        "#         print(\"✓ 沒有 NULL 值\")\n",
        "\n",
        "#     # 統計\n",
        "#     unable_count = sum(submission_df['answer'].str.contains('Unable to answer', na=False))\n",
        "#     multi_ref_count = sum(submission_df['ref_id'].str.count(',') >= 1)\n",
        "\n",
        "#     print(f\"✓ Unable to answer: {unable_count} 題\")\n",
        "#     print(f\"✓ 多文檔引用: {multi_ref_count} 題\")\n",
        "\n",
        "#     submission_df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "#     print(f\"\\n📄 提交文件: {output_path}\")\n",
        "\n",
        "#     generator.print_stats()\n",
        "\n",
        "#     return submission_df\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # 6. 主函數\n",
        "# # ============================================================================\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     WORK_DIR = Path(\"/content/wattbot_working\")\n",
        "#     INPUT_DIR = Path(\"/root/.cache/kagglehub/competitions/WattBot2025\")\n",
        "\n",
        "#     print(\"=\"*70)\n",
        "#     print(\"📂 載入數據...\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     hybrid_chunks_file = \"/content/hybrid_chunks.json\"\n",
        "#     chunks = load_hybrid_chunks(hybrid_chunks_file)\n",
        "\n",
        "#     metadata = pd.read_csv(INPUT_DIR / \"metadata.csv\", encoding=\"latin-1\")\n",
        "#     test_q = pd.read_csv(INPUT_DIR / \"test_Q.csv\", encoding=\"latin-1\")\n",
        "\n",
        "#     metadata.columns = [col.replace('\\ufeff', '').replace('ï»¿', '') for col in metadata.columns]\n",
        "#     test_q.columns = [col.replace('\\ufeff', '').replace('ï»¿', '') for col in test_q.columns]\n",
        "\n",
        "#     print(f\"✓ Metadata: {len(metadata)} 篇\")\n",
        "#     print(f\"✓ Test questions: {len(test_q)} 題\")\n",
        "\n",
        "#     bm25 = build_bm25_index(chunks)\n",
        "\n",
        "#     print(\"\\n🤖 初始化生成器 (保守多文檔版)...\")\n",
        "#     generator = TwoStageGPTGenerator(model=\"gpt-4o-mini\")\n",
        "#     print(\"✓ 完成\")\n",
        "\n",
        "#     submission_df = generate_submission(\n",
        "#         test_df=test_q,\n",
        "#         bm25=bm25,\n",
        "#         chunks=chunks,\n",
        "#         metadata_df=metadata,\n",
        "#         generator=generator,\n",
        "#         output_path=\"/content/submission_conservative_multi_doc.csv\"\n",
        "#     )\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"✅ 完成!\")\n",
        "#     print(\"=\"*70)"
      ],
      "metadata": {
        "id": "kQ83Xpbp6dtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 📊 完整評估結果印出工具 - 直接複製這整個 Cell 到你的 Notebook\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "# ============== 函數 1: 印出單一系統的詳細結果 ==============\n",
        "\n",
        "def print_evaluation_results(results: Dict, title: str = \"評估結果\"):\n",
        "    \"\"\"印出單一系統的完整評估結果\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"📊 {title}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(f\"\\n📈 評估統計:\")\n",
        "    print(f\"  總題數: {results['num_queries']}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"🎯 Recall@K (至少找到一個相關文件的比例)\")\n",
        "    print(\"-\" * 70)\n",
        "    for k in [1, 3, 5, 10]:\n",
        "        recall_val = results[f'Recall@{k}']\n",
        "        print(f\"  Recall@{k:2d}: {recall_val:.4f} ({recall_val:.2%})\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"📊 nDCG@K (考慮排序質量的評估指標)\")\n",
        "    print(\"-\" * 70)\n",
        "    for k in [1, 3, 5, 10]:\n",
        "        ndcg_val = results[f'nDCG@{k}']\n",
        "        print(f\"  nDCG@{k:2d}:  {ndcg_val:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"🏆 MRR (Mean Reciprocal Rank)\")\n",
        "    print(\"-\" * 70)\n",
        "    mrr_val = results['MRR']\n",
        "    avg_position = 1/mrr_val if mrr_val > 0 else float('inf')\n",
        "    print(f\"  MRR: {mrr_val:.4f}\")\n",
        "    if avg_position != float('inf'):\n",
        "        print(f\"  平均第一個相關文件出現在第 {avg_position:.2f} 位\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ============== 函數 2: 比較多個系統 ==============\n",
        "\n",
        "def compare_systems(systems_results: Dict[str, Dict],\n",
        "                   metrics_to_compare: List[str] = None):\n",
        "    \"\"\"比較多個系統的評估結果\"\"\"\n",
        "    if metrics_to_compare is None:\n",
        "        metrics_to_compare = ['Recall@1', 'Recall@3', 'Recall@5', 'MRR', 'nDCG@3', 'nDCG@5']\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"📊 系統比較\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    for metric in metrics_to_compare:\n",
        "        row = {'Metric': metric}\n",
        "\n",
        "        values = {}\n",
        "        for sys_name, results in systems_results.items():\n",
        "            if metric in results:\n",
        "                values[sys_name] = results[metric]\n",
        "                row[sys_name] = f\"{results[metric]:.4f}\"\n",
        "            else:\n",
        "                row[sys_name] = \"N/A\"\n",
        "\n",
        "        if values:\n",
        "            best_val = max(values.values())\n",
        "            for sys_name in values:\n",
        "                if values[sys_name] == best_val:\n",
        "                    row[sys_name] += \" ⭐\"\n",
        "\n",
        "        comparison_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n\" + df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"📈 總結:\")\n",
        "\n",
        "    win_counts = {sys_name: 0 for sys_name in systems_results.keys()}\n",
        "\n",
        "    for metric in metrics_to_compare:\n",
        "        values = {}\n",
        "        for sys_name, results in systems_results.items():\n",
        "            if metric in results:\n",
        "                values[sys_name] = results[metric]\n",
        "\n",
        "        if values:\n",
        "            best_val = max(values.values())\n",
        "            for sys_name, val in values.items():\n",
        "                if val == best_val:\n",
        "                    win_counts[sys_name] += 1\n",
        "\n",
        "    for sys_name, wins in win_counts.items():\n",
        "        print(f\"  • {sys_name}: {wins}/{len(metrics_to_compare)} 個指標最佳\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# ============== 函數 3: 匯出成 CSV ==============\n",
        "\n",
        "def export_metrics_to_csv(systems_results: Dict[str, Dict],\n",
        "                          filename: str = \"evaluation_comparison.csv\"):\n",
        "    \"\"\"將評估結果匯出成 CSV 檔案\"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    for sys_name, results in systems_results.items():\n",
        "        row = {'System': sys_name}\n",
        "\n",
        "        for k in [1, 3, 5, 10]:\n",
        "            row[f'Recall@{k}'] = results.get(f'Recall@{k}', 0)\n",
        "            row[f'nDCG@{k}'] = results.get(f'nDCG@{k}', 0)\n",
        "\n",
        "        row['MRR'] = results.get('MRR', 0)\n",
        "        row['num_queries'] = results.get('num_queries', 0)\n",
        "\n",
        "        all_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n✅ 評估結果已儲存至 {filename}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 🚀 立即使用 - 取消註解下面你需要的部分\n",
        "# ==============================================================================\n",
        "\n",
        "# ===== 範例 1: 印出單一系統結果 =====\n",
        "# print_evaluation_results(metrics_run1, \"Run 1: BM25 Enhanced\")\n",
        "# print_evaluation_results(metrics_hybrid, \"Hybrid Data\")\n",
        "# print_evaluation_results(metrics_text, \"Text Only\")\n",
        "\n",
        "\n",
        "# ===== 範例 2: 比較多個系統 =====\n",
        "# systems = {\n",
        "#     \"BM25 Enhanced\": metrics_run1,\n",
        "#     \"Hybrid Data\": metrics_hybrid,\n",
        "#     \"Text Only\": metrics_text\n",
        "# }\n",
        "# compare_systems(systems)\n",
        "\n",
        "\n",
        "# ===== 範例 3: 匯出成 CSV =====\n",
        "# export_metrics_to_csv(systems, \"my_evaluation_results.csv\")\n",
        "\n",
        "\n",
        "# ===== 完整使用範例 =====\n",
        "\"\"\"\n",
        "在你的 notebook 中，跑完 evaluate_system 之後：\n",
        "\n",
        "# 1. 印出每個系統的詳細結果\n",
        "print_evaluation_results(metrics_run1, \"Run 1: BM25 Enhanced\")\n",
        "print_evaluation_results(metrics_hybrid, \"Hybrid Data\")\n",
        "print_evaluation_results(metrics_text, \"Text Only\")\n",
        "\n",
        "# 2. 比較所有系統\n",
        "systems = {\n",
        "    \"BM25 Enhanced\": metrics_run1,\n",
        "    \"Hybrid Data\": metrics_hybrid,\n",
        "    \"Text Only\": metrics_text\n",
        "}\n",
        "compare_systems(systems)\n",
        "\n",
        "# 3. 匯出結果\n",
        "export_metrics_to_csv(systems, \"evaluation_results.csv\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"✅ 評估工具已載入！\")\n",
        "print(\"💡 取消註解上面的範例程式碼即可使用\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzQ_zD0xAhf2",
        "outputId": "5f62a3a1-a0c0-46de-dfd7-9ab53422112b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 評估工具已載入！\n",
            "💡 取消註解上面的範例程式碼即可使用\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 印出 Run 1 的結果\n",
        "print_evaluation_results(metrics_run1, \"Run 1: BM25 Enhanced\")\n",
        "\n",
        "# 印出 Hybrid Data 的結果\n",
        "print_evaluation_results(metrics_hybrid, \"Hybrid Data\")\n",
        "\n",
        "# 印出 Text Only 的結果\n",
        "print_evaluation_results(metrics_text, \"Text Only\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuUnYHsEAqJe",
        "outputId": "8208f490-7e74-45fd-b7e5-f6b579e1afb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📊 Run 1: BM25 Enhanced\n",
            "======================================================================\n",
            "\n",
            "📈 評估統計:\n",
            "  總題數: 39\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "🎯 Recall@K (至少找到一個相關文件的比例)\n",
            "----------------------------------------------------------------------\n",
            "  Recall@ 1: 0.0000 (0.00%)\n",
            "  Recall@ 3: 0.0000 (0.00%)\n",
            "  Recall@ 5: 0.0000 (0.00%)\n",
            "  Recall@10: 0.0000 (0.00%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "📊 nDCG@K (考慮排序質量的評估指標)\n",
            "----------------------------------------------------------------------\n",
            "  nDCG@ 1:  0.0000\n",
            "  nDCG@ 3:  0.0000\n",
            "  nDCG@ 5:  0.0000\n",
            "  nDCG@10:  0.0000\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "🏆 MRR (Mean Reciprocal Rank)\n",
            "----------------------------------------------------------------------\n",
            "  MRR: 0.0000\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "📊 Hybrid Data\n",
            "======================================================================\n",
            "\n",
            "📈 評估統計:\n",
            "  總題數: 39\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "🎯 Recall@K (至少找到一個相關文件的比例)\n",
            "----------------------------------------------------------------------\n",
            "  Recall@ 1: 0.7949 (79.49%)\n",
            "  Recall@ 3: 0.8974 (89.74%)\n",
            "  Recall@ 5: 0.9231 (92.31%)\n",
            "  Recall@10: 0.9231 (92.31%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "📊 nDCG@K (考慮排序質量的評估指標)\n",
            "----------------------------------------------------------------------\n",
            "  nDCG@ 1:  0.7949\n",
            "  nDCG@ 3:  0.8497\n",
            "  nDCG@ 5:  0.8564\n",
            "  nDCG@10:  0.8617\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "🏆 MRR (Mean Reciprocal Rank)\n",
            "----------------------------------------------------------------------\n",
            "  MRR: 0.8526\n",
            "  平均第一個相關文件出現在第 1.17 位\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "📊 Text Only\n",
            "======================================================================\n",
            "\n",
            "📈 評估統計:\n",
            "  總題數: 39\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "🎯 Recall@K (至少找到一個相關文件的比例)\n",
            "----------------------------------------------------------------------\n",
            "  Recall@ 1: 0.8718 (87.18%)\n",
            "  Recall@ 3: 0.9231 (92.31%)\n",
            "  Recall@ 5: 0.9231 (92.31%)\n",
            "  Recall@10: 0.9487 (94.87%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "📊 nDCG@K (考慮排序質量的評估指標)\n",
            "----------------------------------------------------------------------\n",
            "  nDCG@ 1:  0.8718\n",
            "  nDCG@ 3:  0.8859\n",
            "  nDCG@ 5:  0.8859\n",
            "  nDCG@10:  0.8950\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "🏆 MRR (Mean Reciprocal Rank)\n",
            "----------------------------------------------------------------------\n",
            "  MRR: 0.8974\n",
            "  平均第一個相關文件出現在第 1.11 位\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}